{
 "metadata": {
  "name": "",
  "signature": "sha256:105e1723aecad92bccc6b17f156a3d4c37d5ced961d05b14ae9bfc7f4f2de863"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Problem Definition"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Business Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Home Health Care is a type of health care that is provided to patients in their own home, often after discharge from a hospital.  It ensures that these patients are provided with the necessary care and support to enable them to remain at home and to avoid the stress and expense of further hospitalisation.  A Home Health agency is one that employs a group of healthcare professionals to provide this home-based care to the agency's patients.\n",
      "\n",
      "The Centers for [Medicare and Medicaid Services](http://www.medicare.gov/) (CMS) provide a [Home Health Compare](http://www.medicare.gov/homehealthcompare/compare.html#cmprTab=0&vwgrph=1&cmprID=367787%2C367401&stsltd=OH&loc=45802&lat=40.7111378&lng=-84.1665942) website that enables citizens to compare the performance of the home health agencies in their area. They also publish the raw data that sits behind this website on their [data.medicare.gov](http://data.medicare.gov/data/home-health-compare) site.  This data contains information about the behaviours of each agency and the outcomes obtained by patients served by that agency.\n",
      "\n",
      "For example, for each agency, the dataset contains behavioural information, such as _\"How often the home health team checked patients' risk of falling\"_ along with information on patient outcomes such as _\"How often home health patients had to be admitted to the hospital\"_.  Other types of available outcome data include unplanned trips to the Emergency Room (ER), whether the patient's pain levels improved, etc.\n",
      "\n",
      "Some of these outcomes are hard on the patients and costly for the healthcare system.  So the data presents us with an interesting opportunity:\n",
      "\n",
      "**Could we use the behavioural information to predict which agencies will end up with the worse patient outcomes? (e.g. the highest numbers of hospitalisations)**\n",
      "\n",
      "The CMS is careful to point out that a patient can end up in hospital even when the home health team are providing good care. However, if an agency ends up with a high number of overall hosptial re-admissions, it may also be a sign that all is not well.\n",
      "\n",
      "If we could predict which agencies are likely to end up with the worst patient outcomes, we could better target interventions like  inspections or training. This could improve the lives of many patients while simultaneously reducing the cost of hospital admissions to the healthcare system."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Question we are Aiming to Answer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the dataset, agencies are rated on the outcome _\"How often home health patients had to be admitted to the hospital\"_. Hospital trips are hard on the patients and costly for the healthcare system. So our classifier will try to answer the following question:\n",
      "\n",
      "_\"Can we reliably predict if an agency is likely to end up among the poorest performing agencies for this specific (hospitalisation) outcome?\"_\n",
      "\n",
      "We will define the \"poorest performers\" as those in the tail of the performance distribution, one standard deviation or worse (further) from the mean performance.  \n",
      "\n",
      "In other words - can we train a binary classifier to correctly classify an agency as \"poorest performing\" (binary: true or false) on the basis of their reported behaviours? "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Ethical Considerations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will briefly consider two ethical issues relating to our goal of predicting poor agency performance.\n",
      "\n",
      "Firstly, such predictions should not be used for the *selection* of care agencies for any individual. Selection (or rather non-selection) carries a financial penalty for the agencies in question, but no agency should be penalised on the basis of _potential_ future performance. Rather these predictions could be put to good use in making effective use of training & inspection resources. They could help to identify at-risk agencies and to target interventions wisely - doing so only where there is a good chance that it will improve the outcomes for patients.\n",
      "\n",
      "Secondly, we should take appropriate care with the reputations of the the agencies referred to in the data.  This set of notebooks are primarily an exercise in applying machine learning techniques. However, we will still be dealing with real agencies and we must be respectful of their reputations.  The CMS dataset fully identifies these agencies, with names, addresses and phone numbers. So it would not be fair to publish a data analysis that could be read as saying a given agency is a 'poor performer' (particularly if the probability of classification error is high!). So the identifying information about each agency has been stripped out of the dataset and is neither presented not available."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Measuring Performance"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Choosing the right performance metric"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Accuracy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The default classification performance metric is _accuracy_ - the percentage of predictions that the classifier gets right, regardless of whether it predicts a poor performer or an adequate one.  However, it is not a useful measure of performance for our classifier because our classification labels will be unbalanced.  This means that the number of poorly performing agencies will be sigificantly outnumbered by the number of adequately-performing ones. By our definition, most agencies will fall into the category of 'adequately-performing', so a trivial classifier that simply predicts _\"all agencies will perform adequately\"_ will get a very good accuracy score (it's right most of the time!) _and_ abjectly fail to identify any poor-performing agencies at all!  So we will not use accuracy as our performance metric."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Precision"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A better performance measure for our case is _precision_ - specifically, the precision of the 'poorly performing' label (ignoring the precision of the 'adequately performing' label).\n",
      "\n",
      "Precision measures the proportion of predictions that we get right when we predict _\"this agency will perform poorly\"_. So a precision of 0.8 on the 'poorly performing' label means that if our classifier predicts 100 poorly performing agencies, it will be right for 80 of them and wrong about for 20 ('false positives').\n",
      "\n",
      "High precision gives us confidence that we are *reliably* identifying the poorly performing agencies, so we target inspection & training resources where they will add real value.  We do not want to waste resources on false positives - i.e. agencies that were flagged as potential poor-performers that are in fact doing well for their patients.\n",
      "\n",
      "This could lead us to conclude that precision is the metric to maximise for this problem.  However, while this sounds good on the surface, it is not useful on its own. Consider telling a machine learning 'grid search' to look for a classifier that maximises precision. The search may find a classifier that perfectly identifies poor-performing agencies (never gets it wrong), but only identifies 1% of those agencies. This classifier would be of no value to us. So we need something else to balance it out."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Recall"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The other performance metric that matters to our problem is _recall_. Recall measures the proportion of all poorly performing agencies that our classifier will find. So if there are 100 poorly performing agencies in reality,  a recall of 0.2 means that our classifier will correctly identify 20 of those.\n",
      "\n",
      "For our problem, we need some useful level of recall, but it does not necessarily have to be high to be valuable.  If we could even find 20% of the poorly-performing agencies, we would be able to make a difference."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The best of both worlds - combining Precision and Recall to create a performance target"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So in order to be valuable, what performance will our classifier need to achieve and which measure will we use? \n",
      "\n",
      "We need a balance of recall & precision and scikit-learn provides one off-the-shelf in the form of the [f1-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). However, the f1-score weighs recall and precision the same, which is not good for our scenario. Consider: we need to identify a _useful_ number of _genuine_ poor performers (high precision with low recall). We would not equate that with getting the opposite (i.e. high recall with low precision) as many false positives will waste resources.\n",
      "\n",
      "So instead we change the default weighting of the f-score to account for this by setting a 'beta' parameter. This tells the f-score how much more heavily to weight recall than precision.  For our problem, reliable identification (say precision >= 80%) of even a small proportion of at-risk agencies (say recall >= 20%) would significantly improve the lives of many people.  This (or anything better) would be a useful result - using this to target training and inspections, we could improve many lives by cutting down on hospital admissions , save the healthcare system money and do so with minimal impact and acceptable waste.\n",
      "\n",
      "Precision = 80% and Recall = 20% means that recall is only 1/4 times the importance of precision for our problem. So we set our f-score's beta parameter to 1/4.\n",
      "\n",
      "**In practise, this means that our performance target is an f-score of 0.68.**\n",
      "\n",
      "That said, this is all quite abstract and it can be difficult to get an intuitive sense of how differing values of precision & recall will affect the numbers of true and false positives (in our case - agencies being correctly or incorrectly classified as 'potential poor performers'). To help with this, a '_Deriving the performance metric_' [spreadsheet](./Deriving_the_performance_metric.xlsx) and [screenshot](./Deriving_the_performance_metric.png) are attached. These show the effect of different values of precision & recall on the f-score and (more importantly) on the corresponding, real-world outcomes we would expect to see when classifying 10,000 agencies.  The screenshot is static, but the spreadsheet values can be played with to gain an intuitive understanding."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Formal Definition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\"_ (Mitchell, T., 1997. Machine Learning, McGraw Hill)\n",
      "\n",
      "For this problem then:\n",
      "\n",
      "* **Task (T):** Predict if a home health agency will end up with a high level of patient hospital re-admissions (more than one standard deviation below the mean).\n",
      "\n",
      "* **Experience (E):** Looking at at information about a given home health agency that describes (1) some metrics about the agency's behaviours, (2) some metrics about the outcomes achieved by the agency and (3) a correct classification of those agencies into poor-performing and 'everything else'\n",
      "\n",
      "* **Performance (P):** F-beta (beta=0.25) score, where the threshold of viability is f-beta >= 0.68 (i.e. 80% precision, 20% recall)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Assumptions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this analysis, we will assume:\n",
      "* The samples are independent and identically distributed.\n",
      " * In practise, this is not going to be perfectly true - e.g. a chronically ill individual that goes to one health home (and ends up being re-admitted to hospital) by definition does not go to another health home. Also, the observations may not be identically distributed - e.g. local culture may play a role.\n",
      "* The measures are self-reported and we will assume they are accurate despite the potential for conflict of interest.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table  style='width:100%'>\n",
      "<tr>\n",
      "    <td style='text-align:left; width:33%; border: hidden'>   [<< Table of Contents](./table_of_contents.ipynb)</td>\n",
      "    <td style='text-align:center; width:33%; border: hidden'> </td>\n",
      "    <td style='text-align:right; width:33%; border: hidden'>  [Data Preparation >> ](./data_preparation.ipynb)</td>\n",
      "</tr>\n",
      "</table>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}