{
 "metadata": {
  "name": "",
  "signature": "sha256:5dd33b865092ed7af11c44cc215643d5ba836f66af522d5f1cc8d4a2ab99755c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Problem Definition"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Business Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Home Health Care is a type of health care that is provided to patients in their own home, often after discharge from a hospital.  It ensures that these patients are provided with the necessary care and support to enable them to remain at home and to avoid the stress and expense of further hospitalisation.  A Home Health agency is one that employs a group of healthcare professionals to provide this home-based care to the agency's patients.\n",
      "\n",
      "The Centers for [Medicare and Medicaid Services](http://www.medicare.gov/) (CMS) provide a [Home Health Compare](http://www.medicare.gov/homehealthcompare/compare.html#cmprTab=0&vwgrph=1&cmprID=367787%2C367401&stsltd=OH&loc=45802&lat=40.7111378&lng=-84.1665942) website that enables citizens to compare the performance of the home health agencies in their area. They also publish the raw data that sits behind this website on their [data.medicare.gov](http://data.medicare.gov/data/home-health-compare) site.  This data contains information about the behaviours of each agency and the outcomes obtained by patients served by that agency.\n",
      "\n",
      "For example, for each agency, the dataset contains behavioural information, such as _\"How often the home health team checked patients' risk of falling\"_ along with information on patient outcomes such as _\"How often home health patients had to be admitted to the hospital\"_.  Other types of available outcome data include unplanned trips to the Emergency Room (ER), whether the patient's pain levels improved, etc.\n",
      "\n",
      "Some of these outcomes are hard on the patients and costly for the healthcare system.  So the data presents us with an interesting opportunity:\n",
      "\n",
      "**Could the behavioural information about these agencies be used to predict which agencies will end up with the worst overall patient outcomes? (e.g. those agencies with the highest proportion of hospitalisations)**\n",
      "\n",
      "The CMS is careful to point out that a patient can end up in hospital even when the home health team are providing good care. However, if an agency ends up with a high number of overall hospital re-admissions, it may also be a sign that all is not well.\n",
      "\n",
      "If we could predict which agencies are likely to end up with the worst patient outcomes, we could better target interventions like  inspections or training. This could improve the lives of many patients while simultaneously reducing the cost of hospital admissions to the healthcare system."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Question we are Aiming to Answer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the dataset, agencies are rated on the outcome _\"How often home health patients had to be admitted to the hospital\"_. Hospital trips are hard on the patients and costly for the healthcare system. So our classifier will try to answer the following question:\n",
      "\n",
      "_\"Can we reliably predict if an agency will be among the poorest-performing agencies for patient hospitalisations?\"_\n",
      "\n",
      "We will define the \"poorest performers\" as those in the tail of the performance distribution, one standard deviation or worse (further) from the mean performance.  \n",
      "\n",
      "In other words - can we train a binary classifier to correctly classify an agency as \"poorest performing\" (binary: true or false) on the basis of their reported behaviours? "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Ethical Considerations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will briefly consider two ethical issues relating to our goal of predicting poor agency performance.\n",
      "\n",
      "Firstly, such predictions should not be used for the *selection* of care agencies for any individual. Selection (or rather non-selection) carries a financial penalty for the agencies in question, but no agency should be penalised on the basis of _potential_ future performance. Rather these predictions could be put to good use in making effective use of training & inspection resources. They could help to identify at-risk agencies and to target interventions wisely - doing so only where there is a good chance that it will improve the outcomes for patients.  \n",
      "\n",
      "Secondly, we should take appropriate care with the reputations of the agencies referred to in the data.  This set of notebooks are primarily an exercise in applying machine learning techniques. However, we will still be dealing with real agencies and we must be respectful of their reputations.  The CMS dataset fully identifies these agencies, with names, addresses and phone numbers. So it would not be fair to publish a data analysis that could be read as saying a given agency is a 'poor performer' (particularly if the probability of classification error is high!). So the identifying information about each agency has been stripped out of the dataset and is not presented."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Measuring Performance"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Choosing the right performance metric"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Accuracy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The default classification performance metric is _accuracy_ - the percentage of predictions that the classifier gets right, regardless of whether it predicts a poor performer or an adequate one.  However, it is not a useful measure of performance for our classifier because our classification labels will be unbalanced.  This means that the number of poorly performing agencies will be significantly outnumbered by the number of adequately-performing ones. By our definition, most agencies will fall into the category of 'adequately-performing', so a trivial classifier that simply predicts _\"all agencies will perform adequately\"_ will get a very good accuracy score (it's right most of the time!) _and_ abjectly fail to identify any poor-performing agencies at all!  So we will not use accuracy as our performance metric."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Precision"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A better performance measure for our case is _precision_. Precision measures the proportion of predictions that we get right when we predict _\"this agency will perform poorly\"_.  So a precision of 0.8 on the 'poorly performing' label means that if our classifier predicts 100 poorly performing agencies, it will be right for about 80 of them and wrong for about 20 ('false positives').\n",
      "\n",
      "High precision gives us confidence that we are *reliably* identifying the poorly performing agencies, so we target inspection & training resources where they will add real value.  We do not want to waste resources on false positives - i.e. agencies that were flagged as potential poor-performers that are in fact doing well for their patients.\n",
      "\n",
      "This could lead us to conclude that precision is the metric to maximise for this problem.  However, while this sounds good on the surface, it is not useful on its own. Consider a machine learning optimiser ('grid search') looking for a classifier that maximises precision. It may find a classifier that identifies only a tiny percentage of poor-performing agencies, but always identifies those ones perfectly (precision=1.0). This classifier would be of no value to us. So we need another metric to ensure our classifier is _useful_ in addition to being precise."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Recall"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The other performance metric that matters to our problem is _recall_. Recall measures the proportion of all poorly performing agencies that our classifier will find. So if there are 100 poorly performing agencies in reality,  a recall of 0.2 means that our classifier will correctly identify 20 of those.\n",
      "\n",
      "For our problem, we need some useful level of recall, but it does not necessarily have to be high.  If we could even find 20% of the poorly-performing agencies, we would have a valuable solution that could make a real difference."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The best of both worlds - combining Precision and Recall to create a performance target"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So in order to be valuable, what performance will our classifier need to achieve and which measure will we use? \n",
      "\n",
      "We need a balance of recall & precision and scikit-learn provides one off-the-shelf in the form of the [f1-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). However, the f1-score treats recall and precision as equally important (equal 'weight'), which is not good for our scenario. Consider: we need to identify a _useful_ number of _genuine_ poor performers (high precision with low recall). We would not equate that with getting the opposite (i.e. high recall with low precision) as many false positives will waste resources.\n",
      "\n",
      "To create the metric we need, we will change the weighting used by the f-score by giving it a new 'beta' parameter. 'Beta' tells the f-score how much more importance (weight) to assign to recall than precision.  For our problem, identifying even a small proportion of at-risk agencies (say 20%) would be very good, as long as we could be confident we'd get the prediction right most of the time (say 80%).  Using this to target training and inspections, we could significantly improve the lives of many people by cutting down on hospital admissions, saving the healthcare system money and doing so with minimal impact and acceptable waste.\n",
      "\n",
      "In concrete terms, this means that our performance target is to _\"achieve a precision of 0.8 or better and a recall of 0.2 or better\"_. Putting these together to get an f-score, we note that we have given recall 1/4 of the importance of precision - so we set the 'beta' for our f-score to 1/4.\n",
      "\n",
      "**Our performance target is then \"_achieve an f-score of 0.68, using a beta-adjusted f-score (where beta=0.25)_.\"**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "An intuition for the f-score"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The precision, recall and f-score concepts are rather abstract and it can be difficult to intuitively understand what they mean for our case. What _would_ differing values of these metrics mean for our agencies? Out of 10,000 agencies - how many poor performers would we correctly identify and how many would we get wrong? What if the precision or recall figures were higher or lower?\n",
      "\n",
      "Any value that we may create by developing a classifier is dependent on getting this metric right, so understanding it is critical. To help with this, a '_Deriving the performance metric_' [spreadsheet](./Deriving_the_performance_metric.xlsx) and [screenshot](./Deriving_the_performance_metric.png) are attached. These show the effect of different values of precision & recall on the f-score and (more importantly) on the corresponding, real-world outcomes we would expect to see when classifying 10,000 agencies.  The screenshot is static, but the spreadsheet values can be played with to gain an intuitive understanding."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Formal Definition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\"_ (Mitchell, T., 1997. Machine Learning, McGraw Hill)\n",
      "\n",
      "For this problem then:\n",
      "\n",
      "* **Task (T):** Predict if a home health agency will end up with a high level of patient hospital re-admissions (more than one standard deviation below the mean).\n",
      "\n",
      "* **Experience (E):** Looking at behavioural information about a given home health agency, accompanied by a correct classification of that agency (from which to learn) - i.e. classified as a 'yes or no' (0 or 1) for poor performance.\n",
      "\n",
      "* **Performance (P):** F-beta (beta=0.25) score, where the threshold of viability is f-beta >= 0.68 (i.e. 80% precision, 20% recall)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Assumptions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this analysis, we will assume:\n",
      "* The samples are independent and identically distributed.\n",
      " * In practise, this is not going to be perfectly true - e.g. a chronically ill individual that goes to one health home by definition does not go to another health home. Also, the observations may not be identically distributed - e.g. an agency may have a reputation that attracts a certain type of patient, local culture may play a role, etc.\n",
      "* The measures are self-reported and we will assume they are accurate despite the potential for conflict of interest.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "<table  style='width:100%'>\n",
      "<tr>\n",
      "    <td style='text-align:left; width:33%; border: hidden'>   [<< Table of Contents](./table_of_contents.ipynb)</td>\n",
      "    <td style='text-align:center; width:33%; border: hidden'> </td>\n",
      "    <td style='text-align:right; width:33%; border: hidden'>  [Data Preparation >> ](./data_preparation.ipynb)</td>\n",
      "</tr>\n",
      "</table>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}