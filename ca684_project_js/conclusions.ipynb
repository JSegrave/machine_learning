{
 "metadata": {
  "name": "",
  "signature": "sha256:280d7a689bd3ffeec5b00c8b80725e69b9b54b377c3d18a17fe3abddbf649593"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This project begun with a dataset, a problem and some hope.  We have explored the data in detail and expressed the problem in a concrete, measurable way. However, the hope (that we might be able to predict which home health agencies were likely to perform poorly) has not been realised.  In this conclusion, we will look at possible reasons for this and what other avenues might be worth investigating in future."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Possible Reasons"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "None of our classifiers came close to the target performance threshold of 80% precision with 20% recall (f-score=0.68).  i.e. we cannot make any useful predictions about home health agency performance based on the information available about the behaviour of their staff. \n",
      "\n",
      "What are some of the possible reasons for this?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Our data could be poor quality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All of the behavioural measures in the dataset are self-reported which introduces the possibility of bias and conflict of interest.  Most of the agencies report that they engage in good behaviours nearly all the time - so the  distributions of the behavioural features tend to be squashed up against the top end of the scale.  It might be useful to examine this to determine if it's to be expected, or an indicator of systemic reporting bias (overly-positive)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Our features may not bring out the most distinctive information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perhaps there is some way that we could re-envisage the available features to better highlight the differences between those agencies that perform well and those that perform poorly.  For example, perhaps patients that get to see the doctor more frequently also end up in hospital less. Could we infer from the available features which ones require a doctors input, and thereby improve the performance of our classifiers?  This is a somewhat artificial example, but it speaks to the point that improving classifier performance frequently has more to do with identifying the most informative features than it does with selecting the optimal set of hyperparameters for a particular algorithm."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Our models may be too simple"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many different ways that our model could be overly simplistic:\n",
      "* We may not have tried the right algorithms\n",
      "* We may not have picked the right hyperparameters for the algorithms we have tried\n",
      "* We may be able to break the learning space down into a number of sub-spaces, each of which could be best learned by different types of classifier\n",
      "* etc..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "And finally..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We may already have identified all of the discriminatory information and all of the detectable patterns in the distribution of that information.\n",
      "\n",
      "In other words, we may already have found the best-possible classifier (the Bayes classifier) and even though it is poor, no further improvement in performance is possible."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Opportunities for Further Investigation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some of the possible avenues for further investigation include:\n",
      "* Geocoding the address data for the agencies to see if there are any geographical patterns in the data\n",
      "* Mapping categorical features in the data to multiple new (binary) features.\n",
      " * scikit-learn does not natively work with categorical features. To enable our classifiers to work with categorical features, the usual technique is to convert them to a series of binary features (one per category) using something like [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
      " * We did not do this in our analysis because there was only a single categorical feature in our dataset and one category accounted for 75% of the data. So it did not seem like a promising option.\n",
      "* Merging the home health measures with other CMS data, such as the [Home Health Care Patient Survey](https://data.medicare.gov/Home-Health-Compare/Home-Health-Care-Patient-survey-HHCAHPS-/ccn4-8vby) in which patients rate their home health agencies on various topics. Perhaps some information may exist in those ratings that could provide insight into why some agencies have higher hospitalisation rates.\n",
      "\n",
      "For a little fun, I finished up by uploading my (cleaned) dataset to IBM's Watson Analytics and Amazon's Machine Learning - to see what these cloud-based data analytic services might make of them.\n",
      "\n",
      "IBM's Watson informed me within seconds that it had found a correlation between the behavioural features and the hospitalisation outcome, using a decision tree. However, it noted that it was a very weak correlation that lacked useful predictive power.\n",
      "\n",
      "Amazon's Machine Learning threw an error half way through the data import and would not proceed to predictive analysis. I could not find any means of discovering what caused the error and so had to abandon the attempt."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Concluding Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is always disappointing to reach the end of an investigation without a positive result. However, in this case, the purpose was primarily to arrive at the end with new skills and a deeper understanding of the practise of machine learning.  As is often the case, the failure to identify a solution actually drove more investigation, online research and experimentation - all of which resulted in deeper learning. Taken from that perspective, the project was certainly a success!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "<table  style='width:100%'>\n",
      "<tr>\n",
      "    <td style='text-align:left; width:33%; border: hidden'>  [ << Feature Engineering and New Models](./classifier_modelling_2.ipynb)\n",
      "    <td style='text-align:center; width:33%; border: hidden'> </td>\n",
      "    <td style='text-align:right; width:33%; border: hidden'>  [Table of Contents >> ](./table_of_contents.ipynb)</td>\n",
      "</tr>\n",
      "</table>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}