{
 "metadata": {
  "name": "",
  "signature": "sha256:dc265c82a28efdf7e28db2c246a3ce7cd34282046c7fbf26cb717b7809b14d8e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Feature Engineering and New Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: Jarring start. feed in from last\n",
      "\n",
      "The source data is historical and comes from an external source over which we have no control. So there is no possibility of adding 'net new' information. Instead we will see if any additional features can be engineered from the existing data and then see if these are useful to our baseline classifier.\n",
      "\n",
      "We will also try out other types of classifiers. The code we wrote to train, grid-search and test the K-Nearest Neighbour classifier is re-usable, so we can apply the same techniques to other classifiers. The performance metrics we chose before will also give us a consistent basis for comparison between different classifiers."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "New Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, we will look at adding new features to the data - in the hopes that our classifiers may be able to learn from them.  It should be noted that the addition of these new features could have had an impact on our earlier [Exploratory Data Analysis](./exploratory_data_analysis.ipynb). For example - a new feature may help to identify a clearer decision boundary.\n",
      "\n",
      "While that data analysis was repeated, we shall not reproduce it here. Suffice to say that it looked much the same as before and no new means of separating our labels was discovered."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Non-reporting (Behaviours & Outcomes)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "During the [data preparation](./data_preparation.ipynb) stage, we had to impute values where there were 'nulls' in the original dataset, as the scikit-learn modules would not accept them. Looking more closely at these null values, it turns out that most of them arose where an agency did not report a particular staff behaviour or patient outcome.\n",
      "\n",
      "These null values contain some information about the agency that our models do not yet take into account. Could there be predictive value in understanding the level of non-reporting from a given agency? The behavioural measures are straightforward and all bar one are universally applicable to all kinds of home health agencies - e.g. checking patients for pain, depression, etc (the one exception is a diabetes-specific question). So if an agency did not report some results (or many results), might that correlate with their performance in any way?\n",
      "\n",
      "To explore this possibility, we add two new features to our dataset _'Count of non-reported behaviours'_ and _'Count of non-reported outcomes'_. These attempt to capture the 'magnitude' of non-reporting by an agency. The first feature will count the number of non-reported behaviours for each agency (a number from 0 to 9). The second will do likewise for any non-reported outcomes (again 0 to 9).  We can then check to see if these help improve our classifier's performance. (Note, these new features will be normalised and scaled as before, to ensure that they are weighted fairly by the scikit-learn algorithms)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Footnote text analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking again at the source data, there are 23 separate '_Footnote_' fields - one per feature in the original data. Most of these footnotes are empty, but a few contain unstructured text that may provide us with another source of new information. We can do some simple filtering in Microsoft Excel to look through these & see how many different types there are.\n",
      "\n",
      "<p><img style=\"float: left\" src=\"chronic_and special.png\"></p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It turns out that there are just two:\n",
      "* _\"This agency provides services under a federal waiver program to non-traditional, chronic long term population.\"_\n",
      "* _\"This agency provides services to a special needs population.\"_\n",
      "\n",
      "Unfortunately, as we noticed in the [Data Preparation](./data_preparation.ipynb) stage, the agencies that serve patients with special needs also did not generally report any outcomes. With no labels (and in many cases no behavioural data either) these records were not useful and had to be thrown away.  However, the agencies that provide services to 'non-traditional, chronic' populations do report outcomes (and behaviours).\n",
      "\n",
      "Again, it's possible that this information may correlate with outcomes in some way (i.e. help our classifier to separate labels). So we add another new feature to our dataset _'Non-trad Chronic'_ - a binary variable indicating where an agency provides services to 'non-traditional, chronic' populations.\t"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Binary features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally - there are a number of binary features in the source dataset whose values are supplied as 'TRUE' / 'FALSE' text. Some of the scikit-learn classifiers prefer to see these as binary 0/1 values, so we convert them to that form. This results in the following features becoming available to our classifier:\n",
      "* _'Offers Nursing Care Services'_\n",
      "* _'Offers Physical Therapy Services'_\n",
      "* _'Offers Occupational Therapy Services'_\n",
      "* _'Offers Speech Pathology Services'_\n",
      "* _'Offers Medical Social Services'_\n",
      "* _'Offers Home Health Aide Services'_"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Some Admin..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we load up our new dataset with our new features.\n",
      "\n",
      "The code for training, cross_validating, testing and plotting learning curves was shown in the last section, so we will not repeat it here. Instead we will simply load it in the background using the '%run' command.\n",
      "\n",
      "Also, since this notepad performs grid searches on multiple classifiers using 5-fold cross-validation, it could take a long time to run!  Rather than run it every time, the outputs from each classifier run have been captured as images and embedded under the coprresponding code.  To interact with the code in an IPython notebook, simply load the notebook and ignore the snapshot images - the code will function as normal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline \n",
      "# Re-using the data loading, training, testing and plotting code from before \n",
      "beta=0.25 # our performance metric rates recall being 1/4 as important as precision\n",
      "%run classifier_modelling_2_setup.py\n",
      "%run train_model.py\n",
      "%run grid_search.py\n",
      "# plot_learning_curve() function comes from http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py\n",
      "%run plot_learning_curve.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Final check for our baseline classifier\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Armed with the new features we have just added to the dataset, can our baseline K-Nearest Neighbour classifier do any better than its previous scores? To find out, we will run another grid search to determine the optimal set of hyperparameters, test the classifier against the test data and plot the learning curve.  Here is a reminder of the previous scores on the test data:\n",
      "<pre>\n",
      "f_score:    0.13453\n",
      "precision:  0.13620\n",
      "recall:     0.11243\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "def clf_factory(): return KNeighborsClassifier()\n",
      "def knn_grid_search(debug=False):\n",
      "    grid_hyperparameters = [{'n_neighbors' : range(1, 6), 'weights' : ['uniform', 'distance']}]\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug=debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#knn_grid_search()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for K-Nearest Neighbour</p>**\n",
      "<img style=\"float: left\" src=\"knn_grid_search.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The end for K-Nearest Neighbour..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interestingly, the grid_search now shows that the optimal number of nearest neighbours is 3. However the overasll score has not improved in any meaningful way - the precision has improved marginally, but at the cost of poorer recall.  It is time to see if any other models can improve on our baseline."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "New Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, we will examine the following classification algorithms to see if any can improve on our baseline:\n",
      "* Decision Tree\n",
      "* Support Vector Machine (SVC-RBF)\n",
      "* Support Vector Machine (Polynomial SVC)\n",
      "* Logistic Regression \n",
      "\n",
      "Other classifiers that were also tested include Bernoulli Naive Bayes, Multinomial Naive Bayes and Gaussian Naive Bayes. However, the results from these classifiers were no better than the other and so they are not shown.\n",
      "\n",
      "While techniques like boosting and bagging are for a future project, we do attempt a brief grid search on a Decision Tree using AdaBoost."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Level of Effort**\n",
      "\n",
      "For the purposes of documentation, only one grid-search is shown here for each classification algorithm. However, in practise, each algorithm required several cycles of grid searching, reviewing the selected hyperparameters, refining their ranges and repeating the grid search with the new ranges. This helped in getting a feel for how each classifier works and how the hyperparameters affect the performance.  However, it would be as tedious for the reader to read through these cycles as it was time-consuming to perform them, so the details have been been left out."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Decision Tree"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first classification algorithm we will try is a Decision Tree. Decision Trees have the nice property of being 'white box', meaning that the learned model may give us some insight into our data. If the model performs well, the decision tree can tell us which features mattered most to the model's classification (also known as 'feature importance'). Of course this is only true if the model performs sufficiently well for us to trust it. If the model performs poorly, then the feature importances are of no consequence.\n",
      "\n",
      "As can be seen from the snapshot below, a grid search optimises the 'maximum depth of the tree' hyperparameter.  The result (26% precision, 6% recall) is nearly double the precision of our baseline. However, this performance is still far too poor to be useful in any way - the classifier will only find 6% of the poorly performing agencies, and of those that it classifies as 'poor performers', it will only get it right one in every four times.\n",
      "\n",
      "_Implementation note: Several algorithms have a 'class-weight' mechanism that automatically weights the labels in inverse proportion to their frequency in the dataset. i.e. a minority label will be given more weight than a majority label during classification.  This is a useful option unbalanced datasets like ours. While many classifiers have this option (e.g. Logistic Regression), it was not available on Decision Tree in scikit-learn v0.15 (having only been added recently in v0.16)._"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "def clf_factory(): return DecisionTreeClassifier(random_state=0)\n",
      "def dt_grid_search(debug=False):\n",
      "    grid_hyperparameters = [{\"max_depth\": range(2, 30)}]\n",
      "    #grid_hyperparameters = [{\"max_depth\": range(2, 30), 'class_weight': ['auto', None]}] # can't use auto, as it's a scikit .16 feature\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#dt_grid_search()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for Decision Tree</p>**\n",
      "<img style=\"float: left\" src=\"dt_grid_search.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Support Vector Machine (SVC-RBF)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next classification algorithm we will try is a Support Vector Machine (SVM). SVM is very flexible due to it's ability to use different kernels for different types of classification problem. From our earlier [exploratory data analysis](./exploratory_data_analysis.ipynb), it seemed unlikely that a linear kernel will produce any useful result and indeed this is exactly what the first few cycles of grid searching confirms - SVM with a linear kernel fails to correctly classify any poorly performing agencies.  Likewise the sigmoid kernel also fails to produce any usable results.\n",
      "\n",
      "The Radial Basis Function (rbf) kernel offers more hope, as it can wrap a decision boundary around clusters of data points. So if there are any clusters hiding in our dataset, the RBF kernel may be able to help us discover them.  Fortunately, SVM in scikit-learn also supports automatic class weighting (class_weight=auto), which  is useful for our unbalanced labels.\n",
      "\n",
      "The combination of an RBF kernel, class weighting and repeated cycles of fine-tuning the SVM hyperparameters (C and gamma) produces similar results to the Decision Tree, with slightly better recall.  We end up with a low value for C (which should mean a fairly smooth decision surface) and a very low value for gamma. However, these are of little interest because as with all of our previous classifiers, performance is still nowhere near acceptable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "def clf_factory(): return SVC(class_weight='auto', kernel='rbf')\n",
      "\n",
      "def svc_grid_search(debug=False):\n",
      "    grid_hyperparameters = [{'C':[0.3, 0.35, 0.4], 'gamma':[0.001, 0.005, 0.01]}]\n",
      "    #grid_hyperparameters = [{'gamma':[0.0005, 0.001, 0.005]}]\n",
      "    #grid_hyperparameters = [{'C':[0.3, 0.35, 0.4]}]\n",
      "    #grid_hyperparameters = [{'kernel': ('linear', 'sigmoid', 'rbf')}]\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#svc_grid_search()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for Support Vector Machine (SVC-RBF)</p>**\n",
      "<img style=\"float: left\" src=\"svm_rbf_learning_curve.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Support Vector Machine (Polynomial SVC)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we have touched on the SVC classifier in scikit-learn and tried all of the other kernels (linear, sigmoid and rbf), we will also include SVC's polynomial kernel ('poly') in our grid-search.\n",
      "\n",
      "This is really just for completeness - there is no reason to expect that a polynomial kernel will perform any better than a linear one on this data. Indeed that is exactly what we see - while the precision result on the test dataset looks comparable to previous classifiers, its variance is high. Critically, the recall is minuscule, meaning that it is only identifying a tiny fraction of the poorly performing agencies (possibly just a single case)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "def clf_factory(): return SVC(kernel='poly', degree=2)\n",
      "\n",
      "def svcp_grid_search(debug=False):\n",
      "    grid_hyperparameters = [{'C':[10, 30, 50], 'gamma':[0.01, 0.1]}]\n",
      "    #grid_hyperparameters = [{'C':[0.01, 0.1, 10, 100], 'gamma':[0.01, 0.1, 1]}]\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#svcp_grid_search() # Run time is over an hour"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for Support Vector Machine (Polynomial SVC)</p>**\n",
      "\n",
      "<img style=\"float: left\" src=\"svm_poly_grid_search.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will include Logistic Regression here briefly - as a cautionary tale about data (and a little humour) more than for it's suitability to our classification task.\n",
      "\n",
      "_Context:_ [Weka](http://www.cs.waikato.ac.nz/ml/weka/citing.html) is a very useful workbench for trying out machine learning algorithms quickly. It supports a very broad range of classifiers straight out-of-the-box, and it is straightforward to load data from a .csv file and quickly try out different classification algorithms on it.\n",
      "\n",
      "Having spent a long time with the classifiers above, seeking (in vain) to get the desired precision (0.80) and recall (0.20), I turned to Weka to try out some new classifiers.\n",
      "\n",
      "Among them, I tried out Logistic Regression and was pleasantly surprised to find that its performance was _finally_ heading in the right direction!  While it still was not acceptable, it was getting close:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img style=\"float: left\" src=\"weka_lr.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, all attempts to replicate this result in scikit-learn failed.  After several iterative cycles of grid searches and analysis, I realised that I had inadvertently left some of the \"Outcome\" measurements (the ones used to generate the classification labels) in the dataset! So it wasn't that Weka or Logistic Regression were achieving better results, it was that I had inadvertently handed them a cheat-sheet!\n",
      "\n",
      "Taking a proper look at Logistic Regression (and using the right data!), we can see that it's performance is no better for our use case than previous classifiers.  Interestingly, its recall is much better than anything else we've seen - it will find half of the poor performing agencies. However for each one that it identifies correctly, it will get four more wrong."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "def clf_factory(): return LogisticRegression(C=1e5, class_weight='auto')\n",
      "def lr_grid_search(debug=False):\n",
      "    grid_hyperparameters = {'C': np.logspace(-5, 5, 100), 'class_weight': ['auto', None]}\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#lr_grid_search()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for Logistic Regression</p>**\n",
      "<img style=\"float: left\" src=\"lr_grid_search.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Boosting (AdaBoost with Decision Tree)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, while boosting is beyond the scope of this project, we briefly try it out (ADABoost) to see if we can combine it with one of our weak models (e.g. Decision Tree) to make something stronger.\n",
      "\n",
      "As can be seen from the results below - the boosted 'precision' scores' are in the high 20's, but they come at the price of a much larger standard deviation (i.e. variability)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "\n",
      "def clf_factory(): return AdaBoostClassifier(DecisionTreeClassifier(max_depth=7))\n",
      "\n",
      "def ab_grid_search(debug=False):\n",
      "    grid_hyperparameters = {'n_estimators': [100, 200, 300, 400],\n",
      "                      'base_estimator__max_depth': [1, 5, 10, 15],\n",
      "                      'algorithm': ('SAMME', 'SAMME.R')}\n",
      "    #grid_hyperparameters = {'n_estimators': [100, 200, 300, 400]}\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug) # much more expensive, can only afford 5 folds\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#ab_grid_search(debug=True) # Run time is in excess of an hour"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for AdaBoosted Decision Tree</p>**\n",
      "\n",
      "<pre>\n",
      "Best combination of hyperparameters for this classifier:\n",
      "AdaBoostClassifier(\n",
      "          algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
      "            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=300, random_state=None)\n",
      "\n",
      "f_score:      Mean=0.20188    Stddev=0.08958\n",
      "precision:    Mean=0.28667    Stddev=0.13112\n",
      "recall:       Mean=0.03541    Stddev=0.01498\n",
      "</pre>\n",
      "\n",
      "<img style=\"float: left\" src=\"ab_learning_curve.png\">\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have done a reasonably exhaustive grid-search-based to find good hyperparameters for all of these classifiers. However, none of them have produced any usable results, or anything even approaching a usable result.\n",
      "\n",
      "Why are the results so poor?  It could be because we lack the right model to expose the 'ground truth'. However, we have tried multiple different classifiers and all have come up with much the same precision levels. So it seems more likely that we have reached the 'ground truth' - i.e. there is only a weak connection between the labels we are trying to predict and the behaviours we'd like to base those predictions from.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "<table  style='width:100%'>\n",
      "<tr>\n",
      "    <td style='text-align:left; width:33%; border: hidden'>   [<< Classifier Modelling](./classifier_modelling_1.ipynb)</td>\n",
      "    <td style='text-align:center; width:33%; border: hidden'> [Table of Contents](./table_of_contents.ipynb)</td>\n",
      "    <td style='text-align:right; width:33%; border: hidden'>  [Conclusions >> ](./conclusions.ipynb)</td>\n",
      "</tr>\n",
      "</table>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}