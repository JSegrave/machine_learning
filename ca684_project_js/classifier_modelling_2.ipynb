{
 "metadata": {
  "name": "",
  "signature": "sha256:3ff790da379ba25458d8d3bb919f18beb4ce2e2c5448b9e0a8f97f5f2e95c96a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<font color='red'> TODO: run overnight items below </font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Feature Engineering and New Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "New Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Non-reporting (Behaviours & Outcomes)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Footnote text analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_\"This agency provides services under a federal waiver program to non-traditional, chronic long term population.\"_"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Binary features"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Final check for our baseline classifier\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta=0.25 # our performance metric rates recall being 1/4 as important as precision\n",
      "\n",
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline \n",
      "# Re-using the data loading, training, testing and plotting code from before \n",
      "%run classifier_modelling_2_setup.py\n",
      "%run train_model.py\n",
      "%run grid_search.py\n",
      "# plot_learning_curve() function comes from http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py\n",
      "%run plot_learning_curve.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total samples & features in original dataset: (10106, 44)\n",
        "There are 8965 samples in the dataset (having eliminated samples where the target label was null).\n",
        "There are 1099 poorly-performing agencies labelled in this dataset (label 'Lbl: Poor Hosp Rating 1SD')."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "There are 6275 samples in the training dataset and 2690 in the test dataset."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Poor outcomes make up 12 % of the training data and 12 % of the test dataset.\n",
        "New Features:\n",
        "\tOffers Nursing Care Services\n",
        "\tOffers Physical Therapy Services\n",
        "\tOffers Occupational Therapy Services\n",
        "\tOffers Speech Pathology Services\n",
        "\tOffers Medical Social Services\n",
        "\tOffers Home Health Aide Services\n",
        "\tCount of non-reported behaviours\n",
        "\tCount of non-reported outcomes\n",
        "\tNon-trad Chronic\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "def clf_factory(): return KNeighborsClassifier()\n",
      "def knn_grid_search(debug=False):\n",
      "    grid_hyperparameters = [{'n_neighbors' : range(1, 6), 'weights' : ['uniform', 'distance']}]\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug=debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#knn_grid_search()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for K-Nearest Neighbour</p>**\n",
      "<img style=\"float: left\" src=\"knn_grid_search.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "New Models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Decision Tree"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "def clf_factory(): return DecisionTreeClassifier(random_state=0)\n",
      "def dt_grid_search(debug=False):\n",
      "    grid_hyperparameters = [{\"max_depth\": range(2, 30)}]\n",
      "    #grid_hyperparameters = [{\"max_depth\": range(2, 30), 'class_weight': ['auto', None]}] # can't use auto, as it's a scikit .16 feature\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#dt_grid_search()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for Decision Tree</p>**\n",
      "<img style=\"float: left\" src=\"dt_grid_search.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "def clf_factory(): return LogisticRegression(C=1e5, class_weight='auto')\n",
      "def lr_grid_search(debug=False):\n",
      "    grid_hyperparameters = {'C': np.logspace(-5, 5, 100), 'class_weight': ['auto', None]}\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#lr_grid_search()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for Logistic Regression</p>**\n",
      "<img style=\"float: left\" src=\"lr_grid_search.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Support Vector Machine (Linear SVC)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Can see from the data that a Linear kernel is not going to work, but the Radial Basis Function (rbf) kernel may be more suitable, as may a sigmoid kernel. \n",
      "\n",
      "After some extensive experimentation, it became clear that only the RBF kernel was giving usable results and only when taking the class weights into account (class_weight=auto). so I spent some time working on the values of C & gamma - they have managed to get our precision above 0.2 with little variance, but unfortunately that seems to be the extent of what's possible with an SVM-RBF classifier.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "def clf_factory(): return SVC(class_weight='auto', kernel='rbf')\n",
      "\n",
      "def svc_grid_search(debug=False):\n",
      "    grid_hyperparameters = [{'C':[0.3, 0.35, 0.4], 'gamma':[0.001, 0.005, 0.01]}]\n",
      "    #grid_hyperparameters = [{'gamma':[0.0005, 0.001, 0.005]}]\n",
      "    #grid_hyperparameters = [{'C':[0.3, 0.35, 0.4]}]\n",
      "    #grid_hyperparameters = [{'kernel': ('linear', 'sigmoid', 'rbf')}]\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#svc_grid_search()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for Support Vector Machine (SVC-RBF)</p>**\n",
      "<img style=\"float: left\" src=\"svm_rbf_learning_curve.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Support Vector Machine (Polynomial SVC)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.4.6.1.3. Parameters of the RBF Kernel\n",
      "\n",
      "When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected.\n",
      "Proper choice of C and gamma is critical to the SVM\u2019s performance. One is advised to use sklearn.grid_search.GridSearchCV with C and gamma spaced exponentially far apart to choose good values.\n",
      "http://scikit-learn.org/stable/modules/svm.html#svm-kernels"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "def clf_factory(): return SVC(kernel='poly', degree=2)\n",
      "\n",
      "def svcp_grid_search(debug=False):\n",
      "    grid_hyperparameters = [{'C':[10, 30, 50], 'gamma':[0.01, 0.1]}]\n",
      "    #grid_hyperparameters = [{'C':[0.01, 0.1, 10, 100], 'gamma':[0.01, 0.1, 1]}]\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug)\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#svcp_grid_search() # Run time is over an hour"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: run overnight and add test hyps as above\n",
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for Support Vector Machine (Polynomial SVC)</p>**\n",
      "<pre>\n",
      "Best combination of hyperparameters for this classifier:\n",
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, degree=2, gamma=0.1,\n",
      "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "f_score:      Mean=0.19681    Stddev=0.11846\n",
      "precision:    Mean=0.39619    Stddev=0.22941\n",
      "recall:       Mean=0.02238    Stddev=0.01401\n",
      "</pre>\n",
      "\n",
      "<img style=\"float: left\" src=\"svm_poly_learning_curve.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Boosting (AdaBoost with Decision Tree)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "\n",
      "def clf_factory(): return AdaBoostClassifier(DecisionTreeClassifier(max_depth=7),\n",
      "                         algorithm=\"SAMME\",\n",
      "                         n_estimators=150)\n",
      "\n",
      "def ab_grid_search(debug=False):\n",
      "    grid_hyperparameters = {'n_estimators': [100, 200, 300, 400],\n",
      "                      'base_estimator__max_depth': [1, 5, 10, 15],\n",
      "                      'algorithm': ('SAMME', 'SAMME.R')}\n",
      "    #grid_hyperparameters = {'n_estimators': [100, 200, 300, 400]}\n",
      "    best_clf = grid_search(clf_factory, grid_hyperparameters, samples_train, labels_train, 5)\n",
      "    print best_clf\n",
      "    train_model(best_clf, samples_train, labels_train, 5, beta, debug) # much more expensive, can only afford 5 folds\n",
      "    precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, beta, debug)\n",
      "    plot_learning_curve(best_clf, type(best_clf), samples, labels, ylim=(0, 1), n_folds=5, n_jobs=-1, scoring=scoring)\n",
      "\n",
      "# Run these grid search functions one by one (they take far too long to run all together).\n",
      "# Hence they're commented out and a snapshot of the outputs are captured below\n",
      "#ab_grid_search(debug=True) # Run time is in excess of an hour"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<p>Snapshot of Grid Search, Testing and Learning Curve for AdaBoosted Decision Tree</p>**\n",
      "\n",
      "<pre>\n",
      "Best combination of hyperparameters for this classifier:\n",
      "AdaBoostClassifier(\n",
      "          algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
      "            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=300, random_state=None)\n",
      "\n",
      "f_score:      Mean=0.20188    Stddev=0.08958\n",
      "precision:    Mean=0.28667    Stddev=0.13112\n",
      "recall:       Mean=0.03541    Stddev=0.01498\n",
      "</pre>\n",
      "\n",
      "<img style=\"float: left\" src=\"ab_learning_curve.png\">\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Other classification algorithms that were briefly tried..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random Forest \n",
      "\n",
      "Bernoulli Naive Bayes, Multinomial Naive Bayes and Gaussian Naive Bayes\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}