{
 "metadata": {
  "name": "",
  "signature": "sha256:2907fc91b9fd07110efec33bb87bcfed5101ecdfc5cca167778d39a76fc623da"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifier Modeling: K-Nearest Neighbour"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our goal is to estimate which of two classes an agency will fall into, either:\n",
      "1. Those whose patients will have high rates of hospital admission (i.e. more than one standard deviation above the mean\n",
      "1. 'All the rest'.\n",
      "\n",
      "However, our earlier exploratory analysis provides little guidance about where to start with classifier selection as it did not identify any distribution patterns in the labels that we can exploit. e.g. there is no obvious hyperplane or decision boundary that we can model.\n",
      "\n",
      "To understand where to start, it helps to look at the nature of the source data.  The features we are using are not continuous, they are behavioural ratings that are scored out of 100. Many are also heavily skewed towards the upper end of the range.  They are effectively a set of 100 quantised dots. Any one dot on the scatterplot can represent multiple data points - some poor performers (red) and some not (blue).  As such, there are a lot of overlapping dots, particularly where ther distributions are skewed.  This data is not well suited to _visual_ inspection. If each dot on the scatterplot overwrites previous dots in the same position, it is difficult to get an accurate visual sense of the distribution - as can be seen in the figure below. Seting the dots to be slightly transparent, so we can see where there is a mix, helps a little.\n",
      "\n",
      "![Alt text](./quantised_points.png \"Fig. 1: The data are quantised.\")\n",
      "\n",
      "However, _if_ there are clusters of poorly performing agencies in the data, then the localised voting mechanism of a K-Nearest Neighbour classifier may be able to 'see' that pattern that our eyes cannot.\n",
      "\n",
      "So we will start take a k-nearest neighbour classifier as our initial baseline."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Prepare The Data For Modelling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As before, we:\n",
      "* Load the cleansed data.\n",
      "* Split out the target label we are interested in.\n",
      "* Remove any samples that are missing the target label (our classifier can learn nothing from these)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas\n",
      "import numpy\n",
      "dataset = pandas.read_csv(\"./dataset_v5.csv\")\n",
      "print 'There are %1i samples in the original dataset.' % (dataset.shape[0])\n",
      "\n",
      "target_label = 'Lbl: Poor Hosp Rating 1SD' # Target label: agencies whose rate of hospital admissions is one std dev above the mean\n",
      "\n",
      "# Strip out any rows where there is no label to train or test with\n",
      "dataset = dataset[dataset[target_label].notnull()]\n",
      "\n",
      "# keep the agency and behavior fields we're interested in (exclude outcomes that we're trying to predict, phone numbers, IDs etc)\n",
      "samples = dataset[[\n",
      "    'How often the home health team began their patients care in a timely manner',\n",
      "    'How often the home health team taught patients (or their family caregivers) about their drugs',\n",
      "    'How often the home health team checked patients risk of falling',\n",
      "    'How often the home health team checked patients for depression',\n",
      "    'How often the home health team made sure that their patients have received a flu shot for the current flu season.',\n",
      "    'How often the home health team made sure that their patients have received a pneumococcal vaccine (pneumonia shot).',\n",
      "    'With diabetes - how often the home health team got doctors orders and gave foot care and taught patients about foot care',\n",
      "    'How often the home health team checked patients for pain',\n",
      "    'How often the home health team treated their patients pain',\n",
      "    'How often the home health team took doctor-ordered action to prevent pressure sores (bed sores)',\n",
      "    'How often the home health team checked patients for the risk of developing pressure sores (bed sores)',\n",
      "]]\n",
      "\n",
      "# (1) reshape the label matrix to be flat and (2) change any true/false text into {0,1} as sklearn modules expect\n",
      "labels = numpy.reshape(dataset[target_label].astype(int), -1)\n",
      "print 'There are %1i samples in the dataset (having eliminated samples where the target label was null).' % (dataset.shape[0])\n",
      "print 'There are %1s poorly-performing agencies labelled in this dataset (label \\'%2s\\').' % ((sum(p == 1 for p in labels)), target_label)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 10106 samples in the original dataset.\n",
        "There are 8965 samples in the dataset (having eliminated samples where the target label was null).\n",
        "There are 1099 poorly-performing agencies labelled in this dataset (label 'Lbl: Poor Hosp Rating 1SD')."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Label Balance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have defined the group of 'poor performers' that we wish to identify with our classifier as _\"any agency whose rating for hospital admissions is one standard deviation or more above the mean\"_. As we noted in the [Problem Definition](./problem_definition.ipynb), this makes our dataset unbalanced as can be seen by the figures above. Poor performers are in a minority - 1099 out of 8965 agencies.  We will need to keep an eye on this, as it will affect our choice of performance metrics and means of data sampling."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Many machine learning algorithms are not scale-invariant, so we should pre-process the features to normalise them. This will ensure they are zero-centered with variance of the same order (reference: scikit-learn [Preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "# n.b. this switches samples from a pandas DataFrame to a NumPy array\n",
      "samples = preprocessing.normalize(samples.astype(float))\n",
      "samples = preprocessing.scale(samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Setting Aside Testing Data for Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To avoid over-fitting our classifier to the availabel data, we will break the dataset into a training set and a test set. The training set will be used to train the classifier and the test set will be held back for subsequent evaluation of the classifier's performance.\n",
      "\n",
      "**Balance**\n",
      "\n",
      "As the labels are not balanced in the dataset, we have to be careful that the train & test datasets end up with roughly similar label proportions. While many of the scikit-learn modules offer a 'stratification' option to preserve label proportions, train_test_split() does not. So we will print out the proportions (see below) to check that they roughly match."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# put test data aside\n",
      "from sklearn.cross_validation import train_test_split\n",
      "samples_train, samples_test, labels_train, labels_test = train_test_split(samples,labels, test_size=0.3, random_state=2)\n",
      "percent_train_positives = (100 * (sum(p == 1 for p in labels_train))) // samples_train.shape[0]\n",
      "percent_test_positives = (100 * (sum(p == 1 for p in labels_test))) // samples_test.shape[0]\n",
      "print 'There are %1i samples in the training dataset and %2i in the test dataset.' % (samples_train.shape[0], samples_test.shape[0])\n",
      "print 'Poor outcomes make up %1i %% of the training data and %2i %% of the test dataset.' % (percent_train_positives, percent_test_positives)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 6275 samples in the training dataset and 2690 in the test dataset.\n",
        "Poor outcomes make up 12 % of the training data and 12 % of the test dataset.\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Fitting our First Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can take a first look at how well K-Nearest Neighbour can do on our data. We will use the default hyperparameters to get a baseline measure of performance, e.g. five nearest neighbours. (_aside: hyperparameters are those parameters of the learning model that we set up in advance, as they are not directly learned - e.g. the number of neighbours to look at in a K-nearest neighbour model)_.\n",
      "\n",
      "We will then try to improve on that baseline by performing a grid search to identify better hyperparameters for the classifier.   Armed with better hyperparameters, we will create the corresponding classifier and test it against the test data. Finally, we willsee if our model is under-fitting or over-fitting the data by  examining the classifier's learning curve."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A quick reminder of our target"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall from the [Problem Definition](./problem_definition.ipynb) that we can significantly improve the lives of many people by _reliably_ identifying even a small number of poor-performing agencies in advance. Specifically, we are looking for _precision=0.8_ or better and _recall=0.2_ or better.  Precision is critical, because without it we'll waste inspection & training resources on agencies that would have done well for their patients anyway."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Ensuring our model can generalise - Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To ensure that we correctly interpret the results we get from our classifier, we will run the classifier several times against different samplings of the data (known as 'folds'). This is called 'cross-validating' our classifier and we will perform a 10-fold cross-validation.\n",
      "\n",
      "**Balance**\n",
      "\n",
      "As the classes in our dataset are unbalanced, it's important that the cross validation sampling does not create an extreme imbalance in any of the folds (e.g. a fold containing no poorly performing agencies).  To ensure this, we will use a _stratified_ cross validation rather than the default. The stratification simply ensures that each training & test set retains roughly the same proportion of labels as the original set (for more information, see [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html) and [StratifiedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import StratifiedKFold\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "import numpy\n",
      "\n",
      "# Some of the ideas expressed in this method came from \"Building Machine Learning Systems with Python\" Richert W., Coelho, L., Packt 2013\n",
      "def train_model(clf, samples, labels, folds = 5, debug = False):\n",
      "    samples_df = pandas.DataFrame(samples) # more familiar working with Pandas dataframes\n",
      "    cross_validation = StratifiedKFold(labels, n_folds=folds) # Use Stratified types here due to the imbalance in the labels\n",
      "    precisions, recalls, f_scores = [], [], [] # capture metrics so we can calculate mean and std dev\n",
      "    for train, test in cross_validation:\n",
      "        samples_train, labels_train = samples_df.iloc[train], labels[train] # split out the training set for this fold\n",
      "        samples_test, labels_test = samples_df.iloc[test], labels[test]     # split out the test set for this fold\n",
      "        clf.fit(samples_train, labels_train)\n",
      "        precision, recall, f_score = test_model(clf, samples_test, labels_test, debug)\n",
      "        precisions.append(precision) \n",
      "        recalls.append(recall)\n",
      "        f_scores.append(f_score)     \n",
      "    print(\"f_score:\\t\\tMean=%.5f\\t\\tStddev=%.5f\"%(numpy.mean(f_scores), numpy.std(f_scores)))\n",
      "    print(\"recall:\\t\\t\\tMean=%.5f\\t\\tStddev=%.5f\"%(numpy.mean(recalls), numpy.std(recalls)))\n",
      "    print(\"precision:\\t\\tMean=%.5f\\t\\tStddev=%.5f\"%(numpy.mean(precisions), numpy.std(precisions)))\n",
      "\n",
      "def test_model(clf, samples_test, labels_test, debug = False):\n",
      "        predictions = clf.predict(samples_test)\n",
      "        precision, recall, f_score, support = precision_recall_fscore_support(labels_test, predictions, beta=beta)\n",
      "        # the [1] just captures the score for our minority target label, ignoring the majority label\n",
      "        if debug: print(\"f_score:\\t\\t%.5f\\nrecall:\\t\\t\\t%.5f\\nprecision:\\t\\t%.5f\" % (f_score[1], recall[1], precision[1]))\n",
      "        return precision[1], recall[1], f_score[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import neighbors\n",
      "train_model(neighbors.KNeighborsClassifier(n_neighbors=5), samples, labels, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "f_score:\t\tMean=0.03442\t\tStddev=0.03101\n",
        "recall:\t\t\tMean=0.01456\t\tStddev=0.01299\n",
        "precision:\t\tMean=0.15066\t\tStddev=0.14511\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Cross-Validation Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have a baseline to work from, but it's a dreadful one - the scores above mean that our baseline classifier is unable to pick out the poorly performing agencies:\n",
      "* It only identifies a tiny proportion of them (1%)\n",
      "* Those that it does identify, it nearly always gets wrong (only 15% correct)\n",
      "* Neither score is reliable (both have standard deviations that are nearly as large as the mean)\n",
      "\n",
      "This is poor start - it seems unlikely that there are clusters in the data that can be picked out by K-Nearest Neighbour's localised voting mechanism. That said, we may be able to improve on this result by finding better hyperparameters for the k-nearest neighbour algorithm."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Optimising the hyperparameters - Grid Search"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To find the best hyperparameters for our model we will use the 'Exhaustive Grid Search' technique. This technique involves identifying all the the different possible hyperparameters we wish to test, then training and cross-validating a classifier for every combination of those hyperparameters.  Finally, the grid search will inform us which classifier is the 'best' - i.e. which combination of hyperparameters scores best against some metric that we have defined.\n",
      "\n",
      "In this case, we know our grid_hyperparameters = [{'n_neighbors' : range(1, 6), 'weights' : ['uniform', 'distance']}]\n",
      "best_clf = grid_search(neighbors.KNeighborsClassifier(), grid_hyperparameters, samples_train, labels_train, 10)\n",
      "\n",
      "**Scoring**\n",
      "One of the most important aspects of using a grid search is identifying the right metric - i.e. the one that aligns best with our business goals.  The default metric for grid search is 'accuracy', which does not align at all with our business goals or the shape of our data (we don't care about correctly identifying the majority of agencies that will perform fairly).  So we must define a different metric.\n",
      "\n",
      "As we noted above, we are looking for a metric that takes both precision and recall into account.  The f1 score does this to some extent, but it weights precision and recall equally, whereas our goal is to get precision of 0.8 or better with a recall of 0.2 or better - i.e. recall is only 0.25 as important to us as precision. We can re-define the f-score to take a 'beta' parameter that indicates how much more  importance we attach to recall than to precision (in our case we only attach 1/4 as much, or 0.25).\n",
      "\n",
      "**Balance**\n",
      "Finally, we will use a stratified cross-validation mechanism as before. This ensures that grid search trains classifiers using subsets of the data that retain the appropriate proportion of classes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import fbeta_score, make_scorer\n",
      "\n",
      "beta=0.25\n",
      "scoring = make_scorer(fbeta_score, beta=beta) # favour precision over recall, but use both\n",
      "#scoring = 'precision' # same as f1\n",
      "#beta = 1              # same as f1\n",
      "\n",
      "def grid_search(clf_factory, grid_hyperparameters, samples, labels, folds = 5, scoring=scoring):\n",
      "    cross_val = StratifiedKFold(labels, n_folds=folds) # Use Stratified types here due to the imbalance in the labels\n",
      "    grid_search_with_cross_val = GridSearchCV(clf_factory, grid_hyperparameters, scoring=scoring, cv=cross_val, n_jobs=-1)\n",
      "    %time grid_search_with_cross_val.fit(samples, labels)\n",
      "    best_estimator = grid_search_with_cross_val.best_estimator_\n",
      "    print 'Best combination of hyperparameters for this classifier:'\n",
      "    print best_estimator\n",
      "    return best_estimator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid_hyperparameters = [{'n_neighbors' : range(1, 6), 'weights' : ['uniform', 'distance']}]\n",
      "best_clf = grid_search(neighbors.KNeighborsClassifier(), grid_hyperparameters, samples_train, labels_train, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 7.9 s\n",
        "Best combination of hyperparameters for this classifier:\n",
        "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "           metric_params=None, n_neighbors=1, p=2, weights='uniform')\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Grid search results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our grid search \n",
      "We now have a baseline to work from, but it's a dreadful one - the scores above mean that our baseline classifier is unable to pick out the poorly performing agencies:\n",
      "* It only identifies a tiny proportion of them (1%)\n",
      "* Those that it does identify, it nearly always gets wrong (only 15% correct)\n",
      "* Neither score is reliable (both have standard deviations that are nearly as large as the mean)\n",
      "\n",
      "This is poor start - it seems unlikely that there are clusters in the data that can be picked out by K-Nearest Neighbour's localised voting mechanism. That said, we may be able to improve on this result by finding better hyperparameters for the k-nearest neighbour algorithm.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've found the best estimator on the training set, let's test it against our testing set"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Performance on Test Set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precision, recall, f_score = test_model(best_clf, samples_test, labels_test, True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "f_score:\t\t0.12534\n",
        "recall:\t\t\t0.11243\n",
        "precision:\t\t0.12625\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO - unfortunately, we have made negligible progress -  grid search has identified that the best precision score we can get with KNN is (TODO check hasn't changed) 4% and with minuscule recall - so it will find a tiny number of positives and even then, it will almost always get them wrong.\n",
      "\n",
      "As noted before, even a small precision value like 20% would produce good results as long as recall is sufficiently high (say 80% or above).  It's time to try another classifier..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learning Curve"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "bias or variance?\n",
      "Mention decision boundary & how there's no real way to visualise it in this case\t\t\t"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: Finally, examine the learning curve for this optimised classifier to determine if we are under-fitting or over-fitting the data.  Clearly, this model is heavily biased (TODO: explain why via the graph - see \"This is a typical bias/variance plot\" and \"Illustration of Learning Curves\" [here](http://www.astro.washington.edu/users/vanderplas/Astr599/notebooks/18_IntermediateSklearn)). time to try another a classifier..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline\n",
      "\n",
      "# plot_learning_curve() function comes from http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py\n",
      "%run plot_learning_curve.py\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "\n",
      "def plot_learning(clf, samples, labels, title, scoring, n_iter=5, test_size=0.3, ylim=(0, 1)):\n",
      "    cross_val = StratifiedShuffleSplit(labels, n_iter, test_size, random_state=0)\n",
      "    %time plot_learning_curve(clf, title, samples, labels, ylim, cv=cross_val, n_jobs=-1, scoring=scoring)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning(best_clf, samples, labels, 'K-Nearest Neighbour', scoring, n_iter=5, test_size=0.3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEoCAYAAACpaN3LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nn3wAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x12026438>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 4.74 s\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Disappointing performance from our model so far. The learning curve suggests that more data won't fix the issue, we need a more sophisticated model. What feature engineering could we do?\n",
      "        "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}