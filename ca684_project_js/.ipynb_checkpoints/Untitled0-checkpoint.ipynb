{
 "metadata": {
  "name": "",
  "signature": "sha256:9d349b2f2b93827df77edc447d4d750645a6d0ca1949f01b4edbfd550f9e74b7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline \n",
      "\n",
      "import numpy as np\n",
      "import pandas\n",
      "from matplotlib import pyplot\n",
      "from matplotlib import pylab\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn import preprocessing\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.cross_validation import train_test_split\n",
      "import scipy.stats as stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = pandas.read_csv(\"./dataset_v5.csv\")\n",
      "print 'Total samples & features in original dataset: ' + str(dataset.shape)\n",
      "\n",
      "target_label = 'Lbl: Poor Hosp Rating 2SD'\n",
      "#target_label = 'Lbl: Poor ER Rating 2SD'\n",
      "#target_label = 'Lbl: Poor Pain Rating 2SD'\n",
      "#target_label = 'Lbl: Poor Drugs Rating 2SD'\n",
      "\n",
      "# TODO: remove\n",
      "#dataset = dataset[dataset[target_label] != 1].index # works... now how to select nulls?\n",
      "#dataset = dataset[dataset[target_label].notnull()].index # works... now how to select all the fields, not just this one?\n",
      "#dataset = dataset[dataset[target_label].notnull()] # Works :-)\n",
      "\n",
      "# Strip out any rows where there is no label to train or test with\n",
      "dataset = dataset[dataset[target_label].notnull()]\n",
      "\n",
      "# keep the agency and behavior fields we're interested in (exclude things like patient outcomes, phone numbers, addresses, etc)\n",
      "samples = dataset[[\n",
      "##    'State',\n",
      "##    'CMS Certification Number (CCN)*',\n",
      "##    'Type of Ownership',\n",
      "##    'Offers Nursing Care Services',\n",
      "##    'Offers Physical Therapy Services',\n",
      "##    'Offers Occupational Therapy Services',\n",
      "##    'Offers Speech Pathology Services',\n",
      "##    'Offers Medical Social Services',\n",
      "##    'Offers Home Health Aide Services',\n",
      "    'How often the home health team began their patients care in a timely manner',\n",
      "    'How often the home health team taught patients (or their family caregivers) about their drugs',\n",
      "    'How often the home health team checked patients risk of falling',\n",
      "    'How often the home health team checked patients for depression',\n",
      "    'How often the home health team made sure that their patients have received a flu shot for the current flu season.',\n",
      "    'How often the home health team made sure that their patients have received a pneumococcal vaccine (pneumonia shot).',\n",
      "    'With diabetes - how often the home health team got doctors orders and gave foot care and taught patients about foot care',\n",
      "    'How often the home health team checked patients for pain',\n",
      "    'How often the home health team treated their patients pain',\n",
      "    'How often the home health team took doctor-ordered action to prevent pressure sores (bed sores)',\n",
      "    'How often the home health team checked patients for the risk of developing pressure sores (bed sores)',\n",
      "    'Count of non-reported behaviours',\n",
      "    'Count of non-reported outcomes',\n",
      "##    'Non-trad Chronic'\n",
      "]]\n",
      "\n",
      "# (1) reshape the label matrix to be flat\n",
      "# (2) change the boolean {false, true} values into {0,1} as sklearn modules expect\n",
      "labels = np.reshape(dataset[target_label].astype(int), -1)\n",
      "\n",
      "print 'Target label: [' + target_label + ']'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total samples & features in original dataset: (10106, 44)\n",
        "Target label: [Lbl: Poor Hosp Rating 2SD]\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Pre-process to ensure all features are normalised (centered around zero and have variance in the same order), as per:\n",
      "## http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
      "##   \"Many elements used in the objective function of a learning algorithm (such as the RBF kernel\n",
      "##   of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all\n",
      "##   features are centered around zero and have variance in the same order. If a feature has a\n",
      "##   variance that is orders of magnitude larger that others, it might dominate the objective\n",
      "##   function and make the estimator unable to learn from other features correctly as expected.\"\n",
      "\n",
      "# n.b. TODO: this switches samples from a pandas DataFrame to a NumPy array, so we can't run the same code on it!!\n",
      "samples = preprocessing.normalize(samples.astype(float))\n",
      "samples = preprocessing.scale(samples)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Plotting Functions (for use in later sections)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_General functions for graphs throughout this notebook_"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_labels = [0, 1]\n",
      "markers = ['o', 'o']\n",
      "colours = ['b', 'r']\n",
      "alphas = [0.3, 0.6]\n",
      "\n",
      "def scatterplot(xaxis, yaxis, y, title = '', xlabel = 'X', ylabel = 'Y'):\n",
      "    for label, marker, colour, a in zip(data_labels, markers, colours, alphas):\n",
      "        pyplot.scatter(xaxis[y == label], yaxis[y == label], c=colour, s = 30, cmap=plt.cm.bwr, marker=marker, alpha=a, edgecolors='none')\n",
      "    pyplot.xlabel(xlabel)\n",
      "    pyplot.ylabel(ylabel)\n",
      "    pyplot.title(title)\n",
      "    pyplot.legend(loc='best')\n",
      "    pyplot.show()\n",
      "\n",
      "# Attributed to \"Building Machine Learning Systems with Python\", 2013 Richert, W. and Coelho, L., Packt Publishing\n",
      "def plot_precision_recall_curve(auc_score, precision, recall):\n",
      "    pylab.clf()\n",
      "    pylab.figure(num=None, figsize=(5, 4))\n",
      "    pylab.grid(True)\n",
      "    pylab.fill_between(recall, precision, alpha=0.5)\n",
      "    pylab.plot(recall, precision, lw=1)\n",
      "    pylab.xlim([0.0, 1.0])\n",
      "    pylab.ylim([0.0, 1.0])\n",
      "    pylab.xlabel('Recall')\n",
      "    pylab.ylabel('Precision')\n",
      "    pylab.title('P/R curve (AUC=%0.2f)' % auc_score)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'There are %1i samples in the dataset (having eliminated samples where the target label was null).' % (dataset.shape[0])\n",
      "print 'There are %1s poorly performing agencies identified in the dataset by the label \\'%2s\\'.' % ((sum(p == 1 for p in labels)), target_label)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 8965 samples in the dataset (having eliminated samples where the target label was null).\n",
        "There are 274 poorly performing agencies identified in the dataset by the label 'Lbl: Poor Hosp Rating 2SD'."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# put test data aside\n",
      "samples_train, samples_test, labels_train, labels_test = train_test_split(\n",
      "    samples,labels, test_size=0.5, random_state=2)\n",
      "\n",
      "print 'Samples & features in the training set: ' + str(samples_train.shape)\n",
      "print 'Number of poor outcomes in the training dataset: ' + str(sum(p == 1 for p in labels_train))\n",
      "\n",
      "print 'Samples & features in the test set: ' + str(samples_test.shape)\n",
      "print 'Number of poor outcomes in the test dataset: ' + str(sum(p == 1 for p in labels_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Samples & features in the training set: (4482L, 13L)\n",
        "Number of poor outcomes in the training dataset: 144\n",
        "Samples & features in the test set: (4483L, 13L)\n",
        "Number of poor outcomes in the test dataset: 130"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "def test_classifier(classifier, samples_train, labels_train, samples_test, labels_test):\n",
      "    classifier.fit(samples_train, labels_train)\n",
      "    predictions = classifier.predict(samples_test)\n",
      "    print metrics.classification_report(labels_test, predictions, target_names=['Acceptable','Poor']) # (debugging)\n",
      "    # (for debugging) Print full accuracy, precision & recall scores\n",
      "    #print 'accuracy = ' + str(metrics.accuracy_score(labels_test, predictions))\n",
      "    #print 'precision = ' + str(metrics.precision_score(labels_test, predictions))\n",
      "    #print 'recall = ' + str(metrics.recall_score(labels_test, predictions))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Try KNN - rule out, clearly of no use "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import neighbors\n",
      "knn = neighbors.KNeighborsClassifier(n_neighbors=2)\n",
      "test_classifier(knn, samples_train, labels_train, samples_test, labels_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        " Acceptable       0.97      1.00      0.98      4353\n",
        "       Poor       0.14      0.01      0.01       130\n",
        "\n",
        "avg / total       0.95      0.97      0.96      4483\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "#svc = SVC(kernel=\"poly\", degree=5) # poly takes an order parameter\n",
      "%time svc = SVC(C=1, kernel=\"linear\") # one of linear, poly, rbf, sigmoid, precomputed\n",
      "test_classifier(svc, samples_train, labels_train, samples_test, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 0 ns\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        " Acceptable       0.97      1.00      0.99      4353\n",
        "       Poor       0.00      0.00      0.00       130\n",
        "\n",
        "avg / total       0.94      0.97      0.96      4483\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## n.b. Can't use MultinomialNB on normalised data, as it won't accept negative values\n",
      "#from sklearn.naive_bayes import MultinomialNB\n",
      "#nb_m = MultinomialNB()\n",
      "#test_classifier(nb_m, samples_train, labels_train, samples_test, labels_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "nb_g = GaussianNB()\n",
      "test_classifier(nb_g, samples_train, labels_train, samples_test, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        " Acceptable       0.98      0.92      0.95      4353\n",
        "       Poor       0.09      0.25      0.13       130\n",
        "\n",
        "avg / total       0.95      0.90      0.92      4483\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import BernoulliNB\n",
      "nb_b = BernoulliNB()\n",
      "test_classifier(nb_b, samples_train, labels_train, samples_test, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        " Acceptable       0.97      1.00      0.99      4353\n",
        "       Poor       0.00      0.00      0.00       130\n",
        "\n",
        "avg / total       0.94      0.97      0.96      4483\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "rf = RandomForestClassifier(n_estimators = 2)\n",
      "test_classifier(rf, samples_train, labels_train, samples_test, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        " Acceptable       0.97      1.00      0.98      4353\n",
        "       Poor       0.00      0.00      0.00       130\n",
        "\n",
        "avg / total       0.94      0.97      0.96      4483\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cross-Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "from sklearn.cross_validation import StratifiedKFold\n",
      "\n",
      "# Use Stratified types here due to the imbalance in the labels (to ensure similar proportion of labels in the test & train sets)\n",
      "#cv = StratifiedShuffleSplit(labels, n_iter=10, test_size=0.3, random_state=0)\n",
      "cv = StratifiedKFold(labels, n_folds=10)\n",
      "samples_df = pandas.DataFrame(samples)\n",
      "\n",
      "scores = []\n",
      "pr_scores = []\n",
      "precisions = []\n",
      "recalls = []\n",
      "\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "for train, test in cv:\n",
      "    samples_train, labels_train = samples_df.iloc[train], labels[train]\n",
      "    samples_test, labels_test = samples_df.iloc[test], labels[test]\n",
      "    #clf = GaussianNB()\n",
      "    #clf = MultinomialNB()\n",
      "    clf = neighbors.KNeighborsClassifier(n_neighbors=2)\n",
      "    #clf = SVC(kernel=\"poly\", degree=3, probability=True)\n",
      "    clf.fit(samples_train, labels_train)\n",
      "    test_score = clf.score(samples_test, labels_test)\n",
      "    scores.append(test_score)\n",
      "    proba = clf.predict_proba(samples_test)\n",
      "    precision, recall, pr_thresholds = precision_recall_curve(labels_test, proba[:,1])\n",
      "    pr_scores.append(auc(recall, precision))\n",
      "    precisions.append(precision)\n",
      "    recalls.append(recall)\n",
      "    test_pred = clf.predict(samples_test)\n",
      "    print(\"Accuracy = %f\" %(metrics.accuracy_score(labels_test,test_pred)))\n",
      "    print(\"Precision = %f\" %(metrics.precision_score(labels_test,test_pred)))\n",
      "    print(\"Recall = %f\" %(metrics.recall_score(labels_test,test_pred)))\n",
      "    print(\"F1 score = %f\" %(metrics.f1_score(labels_test,test_pred)))\n",
      "    \n",
      "#print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores, np.std(scores))))\n",
      "print 'scores: ' + str(scores)\n",
      "print 'precision: ' + str(precision)\n",
      "print 'recall: ' + str(recall)\n",
      "print 'pr_thresholds: ' + str(pr_thresholds)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy = 0.968820\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "Accuracy = 0.965440"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "Accuracy = 0.968785"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "Accuracy = 0.967670"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "Accuracy = 0.968750"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "Accuracy = 0.969866"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "Accuracy = 0.967634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "Accuracy = 0.969866"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "Accuracy = 0.967634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "Accuracy = 0.963170"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Precision = 0.000000\n",
        "Recall = 0.000000\n",
        "F1 score = 0.000000\n",
        "scores: [0.9688195991091314, 0.96544035674470452, 0.96878483835005569, 0.967670011148272, 0.96875, 0.9698660714285714, 0.9676339285714286, 0.9698660714285714, 0.9676339285714286, 0.9631696428571429]\n",
        "precision: [ 0.03013393  0.0212766   0.          1.        ]\n",
        "recall: [ 1.          0.07407407  0.          0.        ]\n",
        "pr_thresholds: [ 0.   0.5  1. ]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\metrics.py:1771: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n",
        "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "[0.9688195991091314,\n",
        " 0.96544035674470452,\n",
        " 0.96878483835005569,\n",
        " 0.967670011148272,\n",
        " 0.96875,\n",
        " 0.9698660714285714,\n",
        " 0.9676339285714286,\n",
        " 0.9698660714285714,\n",
        " 0.9676339285714286,\n",
        " 0.9631696428571429]"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Attributed to \"Building Machine Learning Systems with Python\", 2013 Richert, W. and Coelho, L., Packt Publishing, p129\n",
      "from sklearn.metrics import precision_recall_curve, auc\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "\n",
      "import sklearn.ensemble as sk\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "def clf_factory():\n",
      "    clf = GaussianNB()\n",
      "    #clf = MultinomialNB()\n",
      "    #clf = neighbors.KNeighborsClassifier(n_neighbors=2)\n",
      "    #clf = SVC(kernel=\"poly\", degree=3, probability=True)\n",
      "    return clf\n",
      "\n",
      "def train_model(clf_factory, X, Y):\n",
      "  # setting random_state to get deterministic behavior\n",
      "  cross_val = ShuffleSplit(X.shape[0], n_iter=10, test_size=0.3, random_state=0)\n",
      "  scores = []\n",
      "  pr_scores = []\n",
      "  precisions, recalls, thresholds = [], [], []\n",
      "  #have to convert X to a pandas DataFrame, as I don't seem to be able to index it correctly as a numpy array\n",
      "  x_df = pandas.DataFrame(X)\n",
      "  for train, test in cross_val:\n",
      "    X_train, y_train = x_df.iloc[train], Y[train]\n",
      "    X_test, y_test = x_df.iloc[test], Y[test]\n",
      "    clf = clf_factory()\n",
      "    clf.fit(X_train, y_train)\n",
      "    train_score = clf.score(X_train, y_train)\n",
      "    test_score = clf.score(X_test, y_test)\n",
      "    scores.append(test_score)\n",
      "    proba = clf.predict_proba(X_test)\n",
      "    precision, recall, pr_thresholds = precision_recall_curve(y_test, proba[:,1])\n",
      "    pr_scores.append(auc(recall, precision))\n",
      "    precisions.append(precision)\n",
      "    recalls.append(recall)\n",
      "\n",
      "  scores_to_sort = pr_scores\n",
      "  median = np.argsort(scores_to_sort)[len(scores_to_sort) / 2]\n",
      "  summary = (np.mean(scores), np.std(scores), np.mean(pr_scores), np.std(pr_scores))\n",
      "  print \"%.3f\\t%.3f\\t%.3f\\t%.3f\"%summary\n",
      "  plot_precision_recall_curve(pr_scores[median], precisions[median], recalls[median])\n",
      "  summary = (np.mean(scores), np.std(scores), np.mean(pr_scores), np.std(pr_scores))\n",
      "  print \"%.3f\\t%.3f\\t%.3f\\t%.3f\\t\" % summary\n",
      "  print('The classifier has a P/R AUC of score of %0.2f with a standard deviation of %0.4f' %(summary[2:4]))\n",
      "\n",
      "%time train_model(clf_factory, samples, labels)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try out grid search - see p134 of the ML book"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Trying again with different outcomes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "QUESTION 2: \"How often patients receiving home health care needed urgent, unplanned care in the ER without being admitted\". The mean score is 11.985 and the StDev is 4.253, so we'll pick 20.491 (mean + 2 SD) as the threshold for bad performance (again - lower frequency is good here). So in our dataset, 280 agencies (out of 10106) will come out as \"poor performers\" for this outcome.\n",
      "\n",
      "** MAYBE HOLD THESE TWO BACK TO THE END? **\n",
      "\n",
      "QUESTION 3: \"How often patients had less pain when moving around\". The mean score 65.419 and the StDev is 17.163, so we'll pick 31.093 (mean - 2 SD) as the threshold for bad performance (note in this case higher frequency is good). So in our dataset, 420 agencies (out of 10106) will come out as \"poor performers\" for this outcome.\n",
      "\n",
      "QUESTION 4: \"How often patients got better at taking their drugs correctly by mouth\". The mean score is 48.327 and the StDev is 14.724, so we'll pick 18.879 (mean - 2 SD) as the threshold for bad performance (again - higher frequency is good). So in our dataset, 327 agencies (out of 10106) will come out as \"poor performers\" for this outcome.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pull out fearture importances from random forest and bar chart them"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://nbviewer.ipython.org/github/herrfz/dataanalysis/blob/master/assignment2/samsung_data_prediction_submitted.ipynb"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.ensemble as ensemble\n",
      "rfc = ensemble.RandomForestClassifier(n_estimators=300, oob_score=True)\n",
      "%time model = rfc.fit(samples_train, labels_train)\n",
      "\n",
      "test_pred = rfc.predict(samples_test)\n",
      "import sklearn.metrics as metrics\n",
      "print(\"Accuracy = %f\" %(metrics.accuracy_score(labels_test,test_pred)))\n",
      "print(\"Precision = %f\" %(metrics.precision_score(labels_test,test_pred)))\n",
      "print(\"Recall = %f\" %(metrics.recall_score(labels_test,test_pred)))\n",
      "print(\"F1 score = %f\" %(metrics.f1_score(labels_test,test_pred)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Learning Curve (accuracy only, or precision/recall as well?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot_learning_curve() function comes from http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py\n",
      "%run plot_learning_curve.py\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "title = 'Learning Curves (Naive Bayes)'\n",
      "cv = ShuffleSplit(samples.shape[0], n_iter=10, test_size=0.2, random_state=0)\n",
      "\n",
      "clf = GaussianNB()\n",
      "# plot_learning_curve(estimator, title, samples, labels, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
      "%time plot_learning_curve(clf, title, samples, labels, ylim=(0.75, 0.85), cv=cv, n_jobs=-1, scoring='accuracy')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Grid Searches"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from sklearn.svm import SVC\n",
      "#from sklearn.grid_search import GridSearchCV\n",
      "##parameters = {'kernel': ('linear', 'sigmoid', 'rbf'), 'C':[1, 5, 10], 'gamma':[0, 0.001, 0.1]}\n",
      "#parameters = {'C':[1, 5, 10], 'gamma':[0, 0.001, 0.1]}\n",
      "#grid = GridSearchCV(SVC(kernel='poly', degree=5), parameters, scoring='precision', n_jobs=4, cv=4)\n",
      "#grid.fit(samples_train, labels_train)\n",
      "#grid.best_estimator_\n",
      "#grid.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tuned_parameters = [{'n_neighbors' : [1, 2, 3], 'weights' : ['uniform', 'distance']}]\n",
      "#knn = GridSearchCV( neighbors.KNeighborsClassifier(), tuned_parameters, cv=3, n_jobs=-1 )\n",
      "#%time knn.fit(samples_train, labels_train)\n",
      "#\n",
      "#print 'Best parameters set found on development set:'\n",
      "#print knn.best_estimator_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# bookmark\n",
      "\n",
      "# PROPER CORSS VALIDATION\n",
      "# \n",
      "# # from http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html\n",
      "# \n",
      "# from sklearn.svm import SVC\n",
      "# from sklearn.cross_validation import StratifiedKFold\n",
      "# from sklearn.feature_selection import RFECV\n",
      "# from sklearn.datasets import make_classification\n",
      "# \n",
      "# # Build a classification task using 3 informative features\n",
      "# #X, y = make_classification(n_samples=1000, n_features=25, n_informative=3,\n",
      "# #                           n_redundant=2, n_repeated=0, n_classes=8,\n",
      "# #                           n_clusters_per_class=1, random_state=0)\n",
      "# \n",
      "# X = samples_train\n",
      "# y = labels_train\n",
      "# \n",
      "# # Create the RFE object and compute a cross-validated score.\n",
      "# svc = SVC(kernel=\"linear\")\n",
      "# # The \"accuracy\" scoring is proportional to the number of correct\n",
      "# # classifications\n",
      "# %time rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y, 2), scoring='precision')\n",
      "# %time rfecv.fit(X, y)\n",
      "# \n",
      "# print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
      "# \n",
      "# # Plot number of features VS. cross-validation scores\n",
      "# pyplot.figure()\n",
      "# pyplot.xlabel(\"Number of features selected\")\n",
      "# pyplot.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
      "# pyplot.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
      "# pyplot.show()\n",
      "# "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}