{
 "metadata": {
  "name": "",
  "signature": "sha256:f41ee5a4b8957a4539f7e528712acf6ab66758912e4226e09a7a9c966881ac5c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifier Modeling 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Review TODOs:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Review each section & make sure you've actually explained what the section is trying to achieve & not just jumped into it\n",
      "* remove %time \n",
      "* replace our scoring measure - use f3 rather than recall"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline \n",
      "\n",
      "import numpy as np\n",
      "import pandas\n",
      "from matplotlib import pyplot\n",
      "from matplotlib import pylab\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn import preprocessing\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.cross_validation import train_test_split\n",
      "import scipy.stats as stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = pandas.read_csv(\"./dataset_v5.csv\")\n",
      "print 'Total samples & features in original dataset: ' + str(dataset.shape)\n",
      "\n",
      "target_label = 'Lbl: Poor Hosp Rating 2SD'\n",
      "#target_label = 'Lbl: Poor ER Rating 2SD'\n",
      "#target_label = 'Lbl: Poor Pain Rating 2SD'\n",
      "#target_label = 'Lbl: Poor Drugs Rating 2SD'\n",
      "\n",
      "# TODO: remove\n",
      "#dataset = dataset[dataset[target_label] != 1].index # works... now how to select nulls?\n",
      "#dataset = dataset[dataset[target_label].notnull()].index # works... now how to select all the fields, not just this one?\n",
      "#dataset = dataset[dataset[target_label].notnull()] # Works :-)\n",
      "\n",
      "# Strip out any rows where there is no label to train or test with\n",
      "dataset = dataset[dataset[target_label].notnull()]\n",
      "\n",
      "# keep the agency and behavior fields we're interested in (exclude things like patient outcomes, phone numbers, addresses, etc)\n",
      "samples = dataset[[\n",
      "##    'State',\n",
      "##    'CMS Certification Number (CCN)*',\n",
      "##    'Type of Ownership',\n",
      "##    'Offers Nursing Care Services',\n",
      "##    'Offers Physical Therapy Services',\n",
      "##    'Offers Occupational Therapy Services',\n",
      "##    'Offers Speech Pathology Services',\n",
      "##    'Offers Medical Social Services',\n",
      "##    'Offers Home Health Aide Services',\n",
      "    'How often the home health team began their patients care in a timely manner',\n",
      "    'How often the home health team taught patients (or their family caregivers) about their drugs',\n",
      "    'How often the home health team checked patients risk of falling',\n",
      "    'How often the home health team checked patients for depression',\n",
      "    'How often the home health team made sure that their patients have received a flu shot for the current flu season.',\n",
      "    'How often the home health team made sure that their patients have received a pneumococcal vaccine (pneumonia shot).',\n",
      "    'With diabetes - how often the home health team got doctors orders and gave foot care and taught patients about foot care',\n",
      "    'How often the home health team checked patients for pain',\n",
      "    'How often the home health team treated their patients pain',\n",
      "    'How often the home health team took doctor-ordered action to prevent pressure sores (bed sores)',\n",
      "    'How often the home health team checked patients for the risk of developing pressure sores (bed sores)',\n",
      "    'Count of non-reported behaviours',\n",
      "    'Count of non-reported outcomes',\n",
      "##    'Non-trad Chronic'\n",
      "]]\n",
      "\n",
      "# (1) reshape the label matrix to be flat\n",
      "# (2) change the boolean {false, true} values into {0,1} as sklearn modules expect\n",
      "labels = np.reshape(dataset[target_label].astype(int), -1)\n",
      "\n",
      "print 'Target label: [' + target_label + ']'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total samples & features in original dataset: (10106, 44)\n",
        "Target label: [Lbl: Poor Hosp Rating 2SD]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'There are %1i samples in the dataset (having eliminated samples where the target label was null).' % (dataset.shape[0])\n",
      "print 'There are %1s poorly performing agencies identified in the dataset by the label \\'%2s\\'.' % ((sum(p == 1 for p in labels)), target_label)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 8965 samples in the dataset (having eliminated samples where the target label was null).\n",
        "There are 274 poorly performing agencies identified in the dataset by the label 'Lbl: Poor Hosp Rating 2SD'."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Plotting Functions (for use in later sections)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_General functions for graphs throughout this notebook_"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_labels = [0, 1]\n",
      "markers = ['o', 'o']\n",
      "colours = ['b', 'r']\n",
      "alphas = [0.3, 0.6]\n",
      "\n",
      "def scatterplot(xaxis, yaxis, y, title = '', xlabel = 'X', ylabel = 'Y'):\n",
      "    for label, marker, colour, a in zip(data_labels, markers, colours, alphas):\n",
      "        pyplot.scatter(xaxis[y == label], yaxis[y == label], c=colour, s = 30, cmap=plt.cm.bwr, marker=marker, alpha=a, edgecolors='none')\n",
      "    pyplot.xlabel(xlabel)\n",
      "    pyplot.ylabel(ylabel)\n",
      "    pyplot.title(title)\n",
      "    pyplot.legend(loc='best')\n",
      "    pyplot.show()\n",
      "\n",
      "# Attributed to \"Building Machine Learning Systems with Python\", 2013 Richert, W. and Coelho, L., Packt Publishing\n",
      "def plot_precision_recall_curve(auc_score, precision, recall):\n",
      "    pylab.clf()\n",
      "    pylab.figure(num=None, figsize=(5, 4))\n",
      "    pylab.grid(True)\n",
      "    pylab.fill_between(recall, precision, alpha=0.5)\n",
      "    pylab.plot(recall, precision, lw=1)\n",
      "    pylab.xlim([0.0, 1.0])\n",
      "    pylab.ylim([0.0, 1.0])\n",
      "    pylab.xlabel('Recall')\n",
      "    pylab.ylabel('Precision')\n",
      "    pylab.title('P/R curve (AUC=%0.2f)' % auc_score)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "First Classifier - the Baseline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It seems fairly clear from the data analysis that a linear classifier is unlikely to work, so we'll start with a k-nearest neighbour classifier for our first baseline.\n",
      "\n",
      "We'll start with the default setting and see how that fares, using stratified k-fold cross validation.  Once we have a baseline f-score to work from, we will perform a grid search to see if we can improve on that baseline score by finding better hyperparameters.  Finally, once we have found good hyperparameters for this classification algorithm, we will examine the learning curve for that classifier to determine if we are under-fitting or over-fitting the data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A Note on Scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Many machine learning algorithms are not scale-invariant, so we should pre-process the features to normalise them. This will ensure they are zero-centered with variance in the same order (reference: scikit-learn [Preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# n.b. this switches samples from a pandas DataFrame to a NumPy array\n",
      "#samples = preprocessing.normalize(samples.astype(float))\n",
      "#samples = preprocessing.scale(samples)\n",
      "\n",
      "# commented due to MultinomialNB "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cross-Validation on K-Nearest Neighbour"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To ensure that we correctly interpret the results we get from our classifier, we will run the classifier several times against different samples or 'folds' of the data. This is a (10-fold) cross-validation of our classifier and scikit learn provides several means of doing it.\n",
      "\n",
      "As the classes in our dataset are already quite unbalanced, it's very important during cross validation that the imbalance does not end up being extreme in one of the samples (e.g. no samples at all containing poorly performing agencies).  To ensure this does not happen, we will use a stratified cross validation (TODO: pick either StratifiedKFold or StratifiedShuffleSplit) to ensure that each training & test set roughly retains the same proportion of labels as the complete set.  (TODO ref http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html). (TODO ref: \"Building Machine Learning Systems with Python\" p40)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "from sklearn.cross_validation import StratifiedKFold\n",
      "from sklearn.metrics import precision_recall_curve, auc\n",
      "import sklearn.metrics as metrics\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "def test_model(clf, samples_test, labels_test, print_scores = False):\n",
      "        predictions = clf.predict(samples_test)\n",
      "        precision, recall, f_score, support = metrics.precision_recall_fscore_support(labels_test, predictions, beta=0.33)\n",
      "        area = auc(recall, precision)        \n",
      "        if print_scores:\n",
      "            print(\"f_score:\\t\\t%.5f\"%(f_score[1]))       # just capture the score for our minority target label, not the majority label\n",
      "            print(\"recall:\\t\\t\\t%.5f\"%(recall[1]))\n",
      "            print(\"precision:\\t\\t%.5f\"%(precision[1]))\n",
      "            #print(\"AUC:\\t\\t%.5f\"%(area))\n",
      "        return precision[1], recall[1], f_score[1], area # just capture the score for our minority target label, not the majority label\n",
      "\n",
      "def train_model(clf_factory, samples, labels, print_scores = False):\n",
      "    samples_df = pandas.DataFrame(samples) # more familiar working with Pandas dataframes\n",
      "    cross_validation = StratifiedKFold(labels, n_folds=10) # Use Stratified types here due to the imbalance in the labels\n",
      "    precisions, recalls, f_scores, AUCs = [], [], [], []\n",
      "    for train, test in cross_validation:\n",
      "        samples_train, labels_train = samples_df.iloc[train], labels[train]\n",
      "        samples_test, labels_test = samples_df.iloc[test], labels[test]\n",
      "        clf = clf_factory()\n",
      "        %time clf.fit(samples_train, labels_train)\n",
      "        precision, recall, f_score, area = test_model(clf, samples_test, labels_test, print_scores)\n",
      "        precisions.append(precision) \n",
      "        recalls.append(recall)\n",
      "        AUCs.append(area)\n",
      "        f_scores.append(f_score)     \n",
      "    print(\"f_score:\\t\\tMean=%.5f\\t\\tStddev=%.5f\"%(np.mean(f_scores), np.std(f_scores)))\n",
      "    print(\"recall:\\t\\t\\tMean=%.5f\\t\\tStddev=%.5f\"%(np.mean(recalls), np.std(recalls)))\n",
      "    print(\"precision:\\t\\tMean=%.5f\\t\\tStddev=%.5f\"%(np.mean(precisions), np.std(precisions)))\n",
      "    # print(\"AUC:\\t\\tMean=%.5f\\t\\tStddev=%.5f\"%(np.mean(AUCs), np.std(AUCs)))\n",
      "\n",
      "def clf_factory(): return MultinomialNB()\n",
      "train_model(clf_factory, samples, labels)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 5 ms\n",
        "Wall time: 4 ms\n",
        "Wall time: 51 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 55 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 107 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 28 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 5 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 22 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 23 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 4 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "f_score:\t\tMean=0.14773\t\tStddev=0.03550\n",
        "recall:\t\t\tMean=0.14987\t\tStddev=0.05871\n",
        "precision:\t\tMean=0.14972\t\tStddev=0.03827\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Cross-Validation on K-Nearest Neighbour - Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have a baseline to work from - it's the worst possible result, our classifier has zero recall with zero precision!  This is very poor start, but hopefully we will be able to improve on this by performing a grid search to find better hyperparameters for this classification algorithm."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Grid Search on K-Nearest Neighbour"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO - split train & test! Explain Grid Search"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# put test data aside\n",
      "samples_train, samples_test, labels_train, labels_test = train_test_split(\n",
      "    samples,labels, test_size=0.5, random_state=2)\n",
      "\n",
      "print 'Samples & features in the training set: ' + str(samples_train.shape)\n",
      "print 'Number of poor outcomes in the training dataset: ' + str(sum(p == 1 for p in labels_train))\n",
      "\n",
      "print 'Samples & features in the test set: ' + str(samples_test.shape)\n",
      "print 'Number of poor outcomes in the test dataset: ' + str(sum(p == 1 for p in labels_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Samples & features in the training set: (4482L, 13L)\n",
        "Number of poor outcomes in the training dataset: 144\n",
        "Samples & features in the test set: (4483L, 13L)\n",
        "Number of poor outcomes in the test dataset: 130"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "def grid_search(clf_factory, grid_hyperparameters, samples, labels):\n",
      "    cv = StratifiedKFold(labels, n_folds=10) # Use Stratified types here due to the imbalance in the labels\n",
      "    grid_search_with_cross_val = GridSearchCV(clf_factory, grid_hyperparameters, scoring='precision', cv=10, n_jobs=-1)\n",
      "    %time grid_search_with_cross_val.fit(samples, labels)\n",
      "    best_estimator = grid_search_with_cross_val.best_estimator_\n",
      "    print 'Best combination of hyperparameters for this classifier:'\n",
      "    print best_estimator\n",
      "    return best_estimator\n",
      "\n",
      "clf = MultinomialNB()\n",
      "#grid_hyperparameters = [{'n_neighbors' : [1, 2, 3], 'weights' : ['uniform', 'distance']}]\n",
      "grid_hyperparameters = [{'alpha' : (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)}]\n",
      "best_clf = grid_search(clf, grid_hyperparameters, samples_train, labels_train)\n",
      "\n",
      "##parameters = {'kernel': ('linear', 'sigmoid', 'rbf'), 'C':[1, 5, 10], 'gamma':[0, 0.001, 0.1]}\n",
      "#parameters = {'C':[1, 5, 10], 'gamma':[0, 0.001, 0.1]}\n",
      "#grid = GridSearchCV(SVC(kernel='poly', degree=5), parameters, scoring='precision', n_jobs=4, cv=4)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 1.71 s\n",
        "Best combination of hyperparameters for this classifier:\n",
        "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've found the best estimator on the training set, let's test it against our testing set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precision, recall, f_score, area = test_model(best_clf, samples_test, labels_test, True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "f_score:\t\t0.14317\n",
        "recall:\t\t\t0.14615\n",
        "precision:\t\t0.14286\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Grid Search on K-Nearest Neighbour - Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO - unfortunately, we have made negligible progress -  grid search has identified that the best recall score we can get with KNN is 2% - so it will find a tiny number of positives and even then, it will almost always get them wrong.\n",
      "\n",
      "As noted before, even a small precision value like 20% would produce good results as long as recall is sufficiently high (say 80% or above).  It's time to try another classifier..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Learning Curve"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: Finally, examine the learning curve for this optimised classifier to determine if we are under-fitting or over-fitting the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot_learning_curve() function comes from http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py\n",
      "%run plot_learning_curve.py\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "cv = StratifiedShuffleSplit(labels, n_iter=10, test_size=0.3, random_state=0)\n",
      "%time plot_learning_curve(best_clf, 'K-Nearest Neighbour', samples, labels, ylim=(0, 1), cv=cv, n_jobs=-1, scoring='precision')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "def test_classifier(clf, samples_train, labels_train, samples_test, labels_test):\n",
      "    clf.fit(samples_train, labels_train)\n",
      "    predictions = clf.predict(samples_test)\n",
      "    print metrics.classification_report(labels_test, predictions, target_names=['Acceptable','Poor']) # (debugging)\n",
      "    print metrics.precision_recall_fscore_support(labels_test, predictions, beta=0.33)\n",
      "\n",
      "    # (for debugging) Print full accuracy, precision & recall scores\n",
      "    #print 'accuracy = ' + str(metrics.accuracy_score(labels_test, predictions))\n",
      "    #print 'precision = ' + str(metrics.precision_score(labels_test, predictions))\n",
      "    #print 'recall = ' + str(metrics.recall_score(labels_test, predictions))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "svc = SVC(kernel=\"poly\", degree=5) # poly takes an order parameter\n",
      "#svc = SVC(C=1, kernel=\"linear\") # one of linear, poly, rbf, sigmoid, precomputed\n",
      "test_classifier(svc, samples_train, labels_train, samples_test, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## n.b. Can't use MultinomialNB on normalised data, as it won't accept negative values\n",
      "#from sklearn.naive_bayes import MultinomialNB\n",
      "#nb_m = MultinomialNB()\n",
      "#test_classifier(nb_m, samples_train, labels_train, samples_test, labels_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "nb_g = GaussianNB()\n",
      "test_classifier(nb_g, samples_train, labels_train, samples_test, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import BernoulliNB\n",
      "nb_b = BernoulliNB()\n",
      "test_classifier(nb_b, samples_train, labels_train, samples_test, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "rf = RandomForestClassifier(n_estimators = 2)\n",
      "test_classifier(rf, samples_train, labels_train, samples_test, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Attributed to \"Building Machine Learning Systems with Python\", 2013 Richert, W. and Coelho, L., Packt Publishing, p129\n",
      "from sklearn.metrics import precision_recall_curve, auc\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "import sklearn.ensemble as sk\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "def clf_factory():\n",
      "    #clf = GaussianNB()\n",
      "    clf = MultinomialNB()\n",
      "    #clf = neighbors.KNeighborsClassifier(n_neighbors=2)\n",
      "    #clf = SVC(kernel=\"poly\", degree=3, probability=True)\n",
      "    return clf\n",
      "\n",
      "def train_model(clf_factory, X, Y):\n",
      "  # setting random_state to get deterministic behavior\n",
      "  cross_val = ShuffleSplit(X.shape[0], n_iter=10, test_size=0.3, random_state=0)\n",
      "  # cross_validation = StratifiedShuffleSplit(labels, n_iter=10, test_size=0.3, random_state=0)\n",
      "  scores = []\n",
      "  pr_scores = []\n",
      "  precisions, recalls, thresholds = [], [], []\n",
      "  #have to convert X to a pandas DataFrame, as I don't seem to be able to index it correctly as a numpy array\n",
      "  x_df = pandas.DataFrame(X)\n",
      "  for train, test in cross_val:\n",
      "    X_train, y_train = x_df.iloc[train], Y[train]\n",
      "    X_test, y_test = x_df.iloc[test], Y[test]\n",
      "    clf = clf_factory()\n",
      "    clf.fit(X_train, y_train)\n",
      "    predictions = clf.predict(X_test)\n",
      "    \n",
      "    proba = clf.predict_proba(X_test)\n",
      "    precision, recall, pr_thresholds = precision_recall_curve(y_test, proba[:,1])\n",
      "    pr_scores.append(auc(recall, precision))\n",
      "    precisions.append(precision)\n",
      "    recalls.append(recall)\n",
      "\n",
      "  scores_to_sort = pr_scores\n",
      "  median = np.argsort(scores_to_sort)[len(scores_to_sort) / 2]\n",
      "  summary = (np.mean(scores), np.std(scores), np.mean(pr_scores), np.std(pr_scores))\n",
      "  print \"%.3f\\t%.3f\\t%.3f\\t%.3f\"%summary\n",
      "  plot_precision_recall_curve(pr_scores[median], precisions[median], recalls[median])\n",
      "  summary = (np.mean(scores), np.std(scores), np.mean(pr_scores), np.std(pr_scores))\n",
      "  print \"%.3f\\t%.3f\\t%.3f\\t%.3f\\t\" % summary\n",
      "  print('The classifier has a P/R AUC of score of %0.2f with a standard deviation of %0.4f' %(summary[2:4]))\n",
      "\n",
      "%time train_model(clf_factory, samples, labels)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try out grid search - see p134 of the ML book"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Trying again with different outcomes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "QUESTION 2: \"How often patients receiving home health care needed urgent, unplanned care in the ER without being admitted\". The mean score is 11.985 and the StDev is 4.253, so we'll pick 20.491 (mean + 2 SD) as the threshold for bad performance (again - lower frequency is good here). So in our dataset, 280 agencies (out of 10106) will come out as \"poor performers\" for this outcome.\n",
      "\n",
      "** MAYBE HOLD THESE TWO BACK TO THE END? **\n",
      "\n",
      "QUESTION 3: \"How often patients had less pain when moving around\". The mean score 65.419 and the StDev is 17.163, so we'll pick 31.093 (mean - 2 SD) as the threshold for bad performance (note in this case higher frequency is good). So in our dataset, 420 agencies (out of 10106) will come out as \"poor performers\" for this outcome.\n",
      "\n",
      "QUESTION 4: \"How often patients got better at taking their drugs correctly by mouth\". The mean score is 48.327 and the StDev is 14.724, so we'll pick 18.879 (mean - 2 SD) as the threshold for bad performance (again - higher frequency is good). So in our dataset, 327 agencies (out of 10106) will come out as \"poor performers\" for this outcome.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pull out fearture importances from random forest and bar chart them"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://nbviewer.ipython.org/github/herrfz/dataanalysis/blob/master/assignment2/samsung_data_prediction_submitted.ipynb"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.ensemble as ensemble\n",
      "rfc = ensemble.RandomForestClassifier(n_estimators=300, oob_score=True)\n",
      "%time model = rfc.fit(samples_train, labels_train)\n",
      "\n",
      "test_pred = rfc.predict(samples_test)\n",
      "import sklearn.metrics as metrics\n",
      "print(\"Accuracy = %f\" %(metrics.accuracy_score(labels_test,test_pred)))\n",
      "print(\"Precision = %f\" %(metrics.precision_score(labels_test,test_pred)))\n",
      "print(\"Recall = %f\" %(metrics.recall_score(labels_test,test_pred)))\n",
      "print(\"F1 score = %f\" %(metrics.f1_score(labels_test,test_pred)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Learning Curve (accuracy only, or precision/recall as well?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot_learning_curve() function comes from http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py\n",
      "%run plot_learning_curve.py\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "title = 'Learning Curves (Naive Bayes)'\n",
      "cv = ShuffleSplit(samples.shape[0], n_iter=10, test_size=0.2, random_state=0)\n",
      "\n",
      "clf = MultinomialNB()\n",
      "# plot_learning_curve(estimator, title, samples, labels, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
      "%time plot_learning_curve(clf, title, samples, labels, ylim=(0.75, 0.85), cv=cv, n_jobs=-1, scoring='accuracy')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Grid Searches"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from sklearn.svm import SVC\n",
      "#from sklearn.grid_search import GridSearchCV\n",
      "##parameters = {'kernel': ('linear', 'sigmoid', 'rbf'), 'C':[1, 5, 10], 'gamma':[0, 0.001, 0.1]}\n",
      "#parameters = {'C':[1, 5, 10], 'gamma':[0, 0.001, 0.1]}\n",
      "#grid = GridSearchCV(SVC(kernel='poly', degree=5), parameters, scoring='precision', n_jobs=4, cv=4)\n",
      "#grid.fit(samples_train, labels_train)\n",
      "#grid.best_estimator_\n",
      "#grid.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tuned_parameters = [{'n_neighbors' : [1, 2, 3], 'weights' : ['uniform', 'distance']}]\n",
      "#knn = GridSearchCV( neighbors.KNeighborsClassifier(), tuned_parameters, cv=3, n_jobs=-1 )\n",
      "#%time knn.fit(samples_train, labels_train)\n",
      "#\n",
      "#print 'Best parameters set found on development set:'\n",
      "#print knn.best_estimator_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# bookmark\n",
      "\n",
      "# PROPER CORSS VALIDATION\n",
      "# \n",
      "# # from http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html\n",
      "# \n",
      "# from sklearn.svm import SVC\n",
      "# from sklearn.cross_validation import StratifiedKFold\n",
      "# from sklearn.feature_selection import RFECV\n",
      "# from sklearn.datasets import make_classification\n",
      "# \n",
      "# # Build a classification task using 3 informative features\n",
      "# #X, y = make_classification(n_samples=1000, n_features=25, n_informative=3,\n",
      "# #                           n_redundant=2, n_repeated=0, n_classes=8,\n",
      "# #                           n_clusters_per_class=1, random_state=0)\n",
      "# \n",
      "# X = samples_train\n",
      "# y = labels_train\n",
      "# \n",
      "# # Create the RFE object and compute a cross-validated score.\n",
      "# svc = SVC(kernel=\"linear\")\n",
      "# # The \"accuracy\" scoring is proportional to the number of correct\n",
      "# # classifications\n",
      "# %time rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y, 2), scoring='precision')\n",
      "# %time rfecv.fit(X, y)\n",
      "# \n",
      "# print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
      "# \n",
      "# # Plot number of features VS. cross-validation scores\n",
      "# pyplot.figure()\n",
      "# pyplot.xlabel(\"Number of features selected\")\n",
      "# pyplot.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
      "# pyplot.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
      "# pyplot.show()\n",
      "# "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Attributed to: http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import svm, datasets\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "from sklearn.metrics import average_precision_score\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.preprocessing import label_binarize\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n_classes = 2\n",
      "\n",
      "labels_score = best_clf.fit(samples_train, labels_train).decision_function(samples_test)\n",
      "\n",
      "# Compute Precision-Recall and plot curve\n",
      "precision = dict()\n",
      "recall = dict()\n",
      "average_precision = dict()\n",
      "for i in range(n_classes):\n",
      "    precision[i], recall[i], _ = precision_recall_curve(labels_test[:, i],\n",
      "                                                        labels_score[:, i])\n",
      "    average_precision[i] = average_precision_score(labels_test[:, i], labels_score[:, i])\n",
      "\n",
      "# Compute micro-average ROC curve and ROC area\n",
      "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(labels_test.ravel(),\n",
      "    labels_score.ravel())\n",
      "average_precision[\"micro\"] = average_precision_score(labels_test, labels_score,\n",
      "                                                     average=\"micro\")\n",
      "\n",
      "# Plot Precision-Recall curve\n",
      "plt.clf()\n",
      "plt.plot(recall[0], precision[0], label='Precision-Recall curve')\n",
      "plt.xlabel('Recall')\n",
      "plt.ylabel('Precision')\n",
      "plt.ylim([0.0, 1.05])\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.title('Precision-Recall example: AUC={0:0.2f}'.format(average_precision[0]))\n",
      "plt.legend(loc=\"lower left\")\n",
      "plt.show()\n",
      "\n",
      "# Plot Precision-Recall curve for each class\n",
      "plt.clf()\n",
      "plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
      "         label='micro-average Precision-recall curve (area = {0:0.2f})'\n",
      "               ''.format(average_precision[\"micro\"]))\n",
      "for i in range(n_classes):\n",
      "    plt.plot(recall[i], precision[i],\n",
      "             label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
      "                   ''.format(i, average_precision[i]))\n",
      "\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.05])\n",
      "plt.xlabel('Recall')\n",
      "plt.ylabel('Precision')\n",
      "plt.title('Extension of Precision-Recall curve to multi-class')\n",
      "plt.legend(loc=\"lower right\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}